2025-06-13 13:54:33,505 DeMo INFO: Saving model in the path :./RGBNT100-DeMo/aboqnope_20250613_135430
2025-06-13 13:54:33,505 DeMo INFO: Namespace(config_file='configs/RGBNT100/DeMo.yml', fea_cft=0, local_rank=0, opts=['OUTPUT_DIR', './RGBNT100-DeMo/aboqnope_20250613_135430'])
2025-06-13 13:54:33,505 DeMo INFO: Loaded configuration file configs/RGBNT100/DeMo.yml
2025-06-13 13:54:33,505 DeMo INFO: 
MODEL:
  TRANSFORMER_TYPE: 'ViT-B-16'
  STRIDE_SIZE: [ 16, 16 ]
  SIE_CAMERA: True
  DIRECT: 0
  SIE_COE: 1.0
  ID_LOSS_WEIGHT: 0.25
  TRIPLET_LOSS_WEIGHT: 1.0
  GLOBAL_LOCAL: True
  HDM: True
  ATM: False
  HEAD: 8
  FROZEN: False
  CENGJIFUSION: False
  NEWDEFORM: True

INPUT:
  SIZE_TRAIN: [ 128, 256 ]
  SIZE_TEST: [ 128, 256 ]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.5 # random erasing
  PADDING: 10

DATALOADER:
  SAMPLER: 'softmax_triplet'
  NUM_INSTANCE: 16
  NUM_WORKERS: 14

DATASETS:
  NAMES: ('RGBNT100')
  ROOT_DIR: '..'

SOLVER:
  BASE_LR: 0.00035
  WARMUP_ITERS: 5
  MAX_EPOCHS: 30
  OPTIMIZER_NAME: 'Adam'
  GAMMA: 0.1
  IMS_PER_BATCH: 128
  EVAL_PERIOD: 1
  SEED: 1111

TEST:
  IMS_PER_BATCH: 128
  RE_RANKING: 'no'
  WEIGHT: ''
  NECK_FEAT: 'before'
  FEAT_NORM: 'yes'
  MISS: "nothing"

OUTPUT_DIR: '..'




2025-06-13 13:54:33,506 DeMo INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 16
  NUM_WORKERS: 14
  SAMPLER: softmax_triplet
DATASETS:
  NAMES: RGBNT100
  ROOT_DIR: ..
INPUT:
  PADDING: 10
  PIXEL_MEAN: [0.5, 0.5, 0.5]
  PIXEL_STD: [0.5, 0.5, 0.5]
  PROB: 0.5
  RE_PROB: 0.5
  SIZE_TEST: [128, 256]
  SIZE_TRAIN: [128, 256]
MODEL:
  ADAPTER: False
  ATM: False
  ATT_DROP_RATE: 0.0
  CENGJIFUSION: False
  DEVICE: cuda
  DEVICE_ID: 0
  DIRECT: 0
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FROZEN: False
  GLOBAL_LOCAL: True
  HDM: True
  HEAD: 8
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 0.25
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  METRIC_LOSS_TYPE: triplet
  NAME: DeMo
  NECK: bnneck
  NEWDEFORM: True
  NO_MARGIN: True
  PRETRAIN_PATH_T: /path/to/your/vitb_16_224_21k.pth
  PROMPT: False
  SIE_CAMERA: True
  SIE_COE: 1.0
  SIE_VIEW: False
  STRIDE_SIZE: [16, 16]
  TRANSFORMER_TYPE: ViT-B-16
  TRIPLET_LOSS_WEIGHT: 1.0
OUTPUT_DIR: ./RGBNT100-DeMo/aboqnope_20250613_135430
SOLVER:
  BASE_LR: 0.00035
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 60
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.1
  IMS_PER_BATCH: 128
  LARGE_FC_LR: False
  LOG_PERIOD: 10
  MARGIN: 0.3
  MAX_EPOCHS: 30
  MOMENTUM: 0.9
  OPTIMIZER_NAME: Adam
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1111
  STEPS: (40, 70)
  WARMUP_FACTOR: 0.01
  WARMUP_ITERS: 5
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
TEST:
  FEAT: 0
  FEAT_NORM: yes
  IMS_PER_BATCH: 128
  MISS: nothing
  NECK_FEAT: before
  RE_RANKING: no
  WEIGHT: 
=> RGB_IR loaded
Dataset statistics:
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |    50 |     8675 |         8
  query    |    50 |     1715 |         8
  gallery  |    50 |     8575 |         8
  ----------------------------------------
data is ready
cengjifusion: False mxa
rwkvbackbone: False mxa
using Transformer_type: ViT-B-16 as a backbone
Resized position embedding: %s to %s torch.Size([197, 768]) torch.Size([129, 768])
Position embedding resize to height:8 width: 16
Successfully load ckpt!
<All keys matched successfully>
Loading pretrained model from CLIP
camera number is : 8
combineway: adaptiveboqdeformablation mxa
2025-06-13 13:54:38,105 DeMo INFO: combineway: adaptiveboqdeformablation
newdeform: True mxa
UsingDiversity: False mxa
[HAQN Ablation] Using configuration: HQ_QP_MS_AF
[HAQN Ablation] ✓ Hierarchical queries: [64, 57, 51, 44]
[HAQN Ablation] ✓ Layer 0: Hierarchical queries enabled with scale 1.000
[HAQN Ablation] ✓ Layer 0: Query propagation enabled (no adapter needed)
[HAQN Ablation] ✓ Layer 1: Hierarchical queries enabled with scale 1.500
[HAQN Ablation] ✓ Layer 1: Query propagation enabled with adapter (64->57)
[HAQN Ablation] ✓ Layer 2: Hierarchical queries enabled with scale 2.000
[HAQN Ablation] ✓ Layer 2: Query propagation enabled with adapter (57->51)
[HAQN Ablation] ✓ Layer 3: Hierarchical queries enabled with scale 2.500
[HAQN Ablation] ✓ Layer 3: Query propagation enabled with adapter (51->44)
[HAQN Ablation] ✓ Adaptive fusion enabled
[HAQN Ablation] Using configuration: HQ_QP_MS_AF
[HAQN Ablation] ✓ Hierarchical queries: [64, 57, 51, 44]
[HAQN Ablation] ✓ Layer 0: Hierarchical queries enabled with scale 1.000
[HAQN Ablation] ✓ Layer 0: Query propagation enabled (no adapter needed)
[HAQN Ablation] ✓ Layer 1: Hierarchical queries enabled with scale 1.500
[HAQN Ablation] ✓ Layer 1: Query propagation enabled with adapter (64->57)
[HAQN Ablation] ✓ Layer 2: Hierarchical queries enabled with scale 2.000
[HAQN Ablation] ✓ Layer 2: Query propagation enabled with adapter (57->51)
[HAQN Ablation] ✓ Layer 3: Hierarchical queries enabled with scale 2.500
[HAQN Ablation] ✓ Layer 3: Query propagation enabled with adapter (51->44)
[HAQN Ablation] ✓ Adaptive fusion enabled
[HAQN Ablation] Using configuration: HQ_QP_MS_AF
[HAQN Ablation] ✓ Hierarchical queries: [64, 57, 51, 44]
[HAQN Ablation] ✓ Layer 0: Hierarchical queries enabled with scale 1.000
[HAQN Ablation] ✓ Layer 0: Query propagation enabled (no adapter needed)
[HAQN Ablation] ✓ Layer 1: Hierarchical queries enabled with scale 1.500
[HAQN Ablation] ✓ Layer 1: Query propagation enabled with adapter (64->57)
[HAQN Ablation] ✓ Layer 2: Hierarchical queries enabled with scale 2.000
[HAQN Ablation] ✓ Layer 2: Query propagation enabled with adapter (57->51)
[HAQN Ablation] ✓ Layer 3: Hierarchical queries enabled with scale 2.500
[HAQN Ablation] ✓ Layer 3: Query propagation enabled with adapter (51->44)
[HAQN Ablation] ✓ Adaptive fusion enabled
[HAQN Ablation] Using configuration: HQ_QP_MS_AF
[HAQN Ablation] ✓ Hierarchical queries: [76, 68, 60, 53]
[HAQN Ablation] ✓ Layer 0: Hierarchical queries enabled with scale 1.000
[HAQN Ablation] ✓ Layer 0: Query propagation enabled (no adapter needed)
[HAQN Ablation] ✓ Layer 1: Hierarchical queries enabled with scale 1.500
[HAQN Ablation] ✓ Layer 1: Query propagation enabled with adapter (76->68)
[HAQN Ablation] ✓ Layer 2: Hierarchical queries enabled with scale 2.000
[HAQN Ablation] ✓ Layer 2: Query propagation enabled with adapter (68->60)
[HAQN Ablation] ✓ Layer 3: Hierarchical queries enabled with scale 2.500
[HAQN Ablation] ✓ Layer 3: Query propagation enabled with adapter (60->53)
[HAQN Ablation] ✓ Adaptive fusion enabled
[HAQN Ablation] Using configuration: HQ_QP_MS_AF
[HAQN Ablation] ✓ Hierarchical queries: [76, 68, 60, 53]
[HAQN Ablation] ✓ Layer 0: Hierarchical queries enabled with scale 1.000
[HAQN Ablation] ✓ Layer 0: Query propagation enabled (no adapter needed)
[HAQN Ablation] ✓ Layer 1: Hierarchical queries enabled with scale 1.500
[HAQN Ablation] ✓ Layer 1: Query propagation enabled with adapter (76->68)
[HAQN Ablation] ✓ Layer 2: Hierarchical queries enabled with scale 2.000
[HAQN Ablation] ✓ Layer 2: Query propagation enabled with adapter (68->60)
[HAQN Ablation] ✓ Layer 3: Hierarchical queries enabled with scale 2.500
[HAQN Ablation] ✓ Layer 3: Query propagation enabled with adapter (60->53)
[HAQN Ablation] ✓ Adaptive fusion enabled
[HAQN Ablation] Using configuration: HQ_QP_MS_AF
[HAQN Ablation] ✓ Hierarchical queries: [76, 68, 60, 53]
[HAQN Ablation] ✓ Layer 0: Hierarchical queries enabled with scale 1.000
[HAQN Ablation] ✓ Layer 0: Query propagation enabled (no adapter needed)
[HAQN Ablation] ✓ Layer 1: Hierarchical queries enabled with scale 1.500
[HAQN Ablation] ✓ Layer 1: Query propagation enabled with adapter (76->68)
[HAQN Ablation] ✓ Layer 2: Hierarchical queries enabled with scale 2.000
[HAQN Ablation] ✓ Layer 2: Query propagation enabled with adapter (68->60)
[HAQN Ablation] ✓ Layer 3: Hierarchical queries enabled with scale 2.500
[HAQN Ablation] ✓ Layer 3: Query propagation enabled with adapter (60->53)
[HAQN Ablation] ✓ Adaptive fusion enabled
[HAQN Ablation] Using configuration: HQ_QP_MS_AF
[HAQN Ablation] ✓ Hierarchical queries: [96, 86, 76, 67]
[HAQN Ablation] ✓ Layer 0: Hierarchical queries enabled with scale 1.000
[HAQN Ablation] ✓ Layer 0: Query propagation enabled (no adapter needed)
[HAQN Ablation] ✓ Layer 1: Hierarchical queries enabled with scale 1.500
[HAQN Ablation] ✓ Layer 1: Query propagation enabled with adapter (96->86)
[HAQN Ablation] ✓ Layer 2: Hierarchical queries enabled with scale 2.000
[HAQN Ablation] ✓ Layer 2: Query propagation enabled with adapter (86->76)
[HAQN Ablation] ✓ Layer 3: Hierarchical queries enabled with scale 2.500
[HAQN Ablation] ✓ Layer 3: Query propagation enabled with adapter (76->67)
[HAQN Ablation] ✓ Adaptive fusion enabled
[HAQN Ablation] ✓ Truly modality-specific: 7 independent BoQ modules
[EDA Ablation] Using configuration: AMW_MSOF_LTS_ROC
[EDA Ablation] ✓ AMW (Adaptive Modal Weighting) enabled
[EDA Ablation] ✓ MSOF (Multi-Scale Offset Fusion) enabled
[EDA Ablation] ✓ LTS (Learnable Temperature Scaling) enabled
[EDA Ablation] ✓ ROC (Residual Offset Connection) enabled
===========Building DeMo===========
2025-06-13 13:54:39,245 DeMo INFO: DeMo(
  (BACKBONE): build_transformer(
    (base): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (pool): AdaptiveAvgPool1d(output_size=1)
  (rgb_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (nir_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (tir_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (generalFusion): GeneralFusion(
    (modalityboqablation): TrulyModalitySpecificBoQAblation(
      (rgb_boq): AdaptiveBoQAblation(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncodingAblation()
        (boqs): ModuleList(
          (0): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=64, out_features=57, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=57, out_features=51, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=51, out_features=44, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusionAblation(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=216, out_features=1, bias=True)
        )
      )
      (ni_boq): AdaptiveBoQAblation(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncodingAblation()
        (boqs): ModuleList(
          (0): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=64, out_features=57, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=57, out_features=51, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=51, out_features=44, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusionAblation(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=216, out_features=1, bias=True)
        )
      )
      (ti_boq): AdaptiveBoQAblation(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncodingAblation()
        (boqs): ModuleList(
          (0): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=64, out_features=57, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=57, out_features=51, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=51, out_features=44, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusionAblation(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=216, out_features=1, bias=True)
        )
      )
      (rgb_ni_boq): AdaptiveBoQAblation(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncodingAblation()
        (boqs): ModuleList(
          (0): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=76, out_features=68, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=68, out_features=60, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=60, out_features=53, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusionAblation(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=257, out_features=1, bias=True)
        )
      )
      (rgb_ti_boq): AdaptiveBoQAblation(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncodingAblation()
        (boqs): ModuleList(
          (0): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=76, out_features=68, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=68, out_features=60, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=60, out_features=53, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusionAblation(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=257, out_features=1, bias=True)
        )
      )
      (ni_ti_boq): AdaptiveBoQAblation(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncodingAblation()
        (boqs): ModuleList(
          (0): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=76, out_features=68, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=68, out_features=60, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=60, out_features=53, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusionAblation(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=257, out_features=1, bias=True)
        )
      )
      (rgb_ni_ti_boq): AdaptiveBoQAblation(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncodingAblation()
        (boqs): ModuleList(
          (0): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=96, out_features=86, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=86, out_features=76, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=76, out_features=67, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusionAblation(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=325, out_features=1, bias=True)
        )
      )
    )
    (deformselectablation): DAttentionEnhancedAblation(
      (modal_gate): Sequential(
        (0): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))
        (3): Sigmoid()
      )
      (conv_offset): Sequential(
        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), groups=512)
        (3): GELU(approximate='none')
        (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (conv_offset_coarse): Sequential(
        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(512, 512, kernel_size=(6, 6), stride=(2, 2), padding=(1, 1), groups=512)
        (3): GELU(approximate='none')
        (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (conv_offset_fine): Sequential(
        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2), groups=512)
        (3): GELU(approximate='none')
        (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (proj_q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attn_drop): Dropout(p=0.0, inplace=False)
    )
  )
  (classifier_moe): Linear(in_features=3584, out_features=50, bias=False)
  (bottleneck_moe): BatchNorm1d(3584, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier_r): Linear(in_features=512, out_features=50, bias=False)
  (bottleneck_r): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier_n): Linear(in_features=512, out_features=50, bias=False)
  (bottleneck_n): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier_t): Linear(in_features=512, out_features=50, bias=False)
  (bottleneck_t): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
2025-06-13 13:54:39,248 DeMo INFO: number of parameters:247.590725
using soft triplet loss for training
label smooth on, numclasses: 50
2025-06-13 13:54:39,482 DeMo.train INFO: start training
diversityweight: 0 mxa
/home/ma1/anaconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/ma1/anaconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/ma1/anaconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ma1/work/demorelated/DeMo/modeling/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
/home/ma1/anaconda3/envs/DeMo/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2025-06-13 13:54:53,248 DeMo.train INFO: Epoch[1] Iteration[10/65] Loss: 9.632, Acc: 0.111, Base Lr: 9.80e-05
2025-06-13 13:55:00,430 DeMo.train INFO: Epoch[1] Iteration[20/65] Loss: 8.351, Acc: 0.158, Base Lr: 9.80e-05
2025-06-13 13:55:07,604 DeMo.train INFO: Epoch[1] Iteration[30/65] Loss: 7.856, Acc: 0.209, Base Lr: 9.80e-05
2025-06-13 13:55:14,944 DeMo.train INFO: Epoch[1] Iteration[40/65] Loss: 7.580, Acc: 0.265, Base Lr: 9.80e-05
2025-06-13 13:55:22,129 DeMo.train INFO: Epoch[1] Iteration[50/65] Loss: 7.396, Acc: 0.319, Base Lr: 9.80e-05
2025-06-13 13:55:29,498 DeMo.train INFO: Epoch[1] Iteration[60/65] Loss: 7.256, Acc: 0.368, Base Lr: 9.80e-05
2025-06-13 13:55:32,037 DeMo.train INFO: Epoch 1 done. Time per batch: 0.832[s] Speed: 153.8[samples/s]
2025-06-13 13:55:32,042 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 13:55:32,042 DeMo.train INFO: Current is the ori feature testing!
2025-06-13 13:55:32,042 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.5670
CMC curve: Rank-1: 0.8280, Rank-5: 0.8601, Rank-10: 0.8810
=> Enter reranking
=> Results after reranking:
mAP: 0.5775
CMC curve: Rank-1: 0.8332, Rank-5: 0.8461, Rank-10: 0.8554
=> Improvement:
mAP improvement: 0.0106 (+1.87%)
Rank-1 improvement: 0.0052 (+0.63%)
tstpic False mxa
2025-06-13 13:57:32,454 DeMo.train INFO: Validation Results - Epoch: 1
2025-06-13 13:57:32,455 DeMo.train INFO: mAP: 57.8%
2025-06-13 13:57:32,455 DeMo.train INFO: CMC curve, Rank-1  :83.3%
2025-06-13 13:57:32,455 DeMo.train INFO: CMC curve, Rank-5  :84.6%
2025-06-13 13:57:32,455 DeMo.train INFO: CMC curve, Rank-10 :85.5%
2025-06-13 13:57:32,455 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 13:57:32,588 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 13:57:32,588 DeMo.train INFO: Current is the moe feature testing!
2025-06-13 13:57:32,588 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/ma1/work/demorelated/DeMo/utils/reranking.py:40: UserWarning: This overload of addmm_ is deprecated:
	addmm_(Number beta, Number alpha, Tensor mat1, Tensor mat2)
Consider using one of the following signatures instead:
	addmm_(Tensor mat1, Tensor mat2, *, Number beta, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)
  distmat.addmm_(1, -2, feat, feat.t())
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.5626
CMC curve: Rank-1: 0.8041, Rank-5: 0.8443, Rank-10: 0.8589
=> Enter reranking
=> Results after reranking:
mAP: 0.5704
CMC curve: Rank-1: 0.8163, Rank-5: 0.8297, Rank-10: 0.8397
=> Improvement:
mAP improvement: 0.0078 (+1.39%)
Rank-1 improvement: 0.0122 (+1.52%)
tstpic False mxa
2025-06-13 13:59:33,147 DeMo.train INFO: Validation Results - Epoch: 1
2025-06-13 13:59:33,148 DeMo.train INFO: mAP: 57.0%
2025-06-13 13:59:33,148 DeMo.train INFO: CMC curve, Rank-1  :81.6%
2025-06-13 13:59:33,148 DeMo.train INFO: CMC curve, Rank-5  :83.0%
2025-06-13 13:59:33,148 DeMo.train INFO: CMC curve, Rank-10 :84.0%
2025-06-13 13:59:33,148 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 13:59:33,164 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 13:59:33,164 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-13 13:59:33,164 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.5853
CMC curve: Rank-1: 0.8286, Rank-5: 0.8612, Rank-10: 0.8799
=> Enter reranking
=> Results after reranking:
mAP: 0.5908
CMC curve: Rank-1: 0.8367, Rank-5: 0.8443, Rank-10: 0.8519
=> Improvement:
mAP improvement: 0.0054 (+0.93%)
Rank-1 improvement: 0.0082 (+0.99%)
tstpic False mxa
2025-06-13 14:01:32,094 DeMo.train INFO: Validation Results - Epoch: 1
2025-06-13 14:01:32,094 DeMo.train INFO: mAP: 59.1%
2025-06-13 14:01:32,094 DeMo.train INFO: CMC curve, Rank-1  :83.7%
2025-06-13 14:01:32,095 DeMo.train INFO: CMC curve, Rank-5  :84.4%
2025-06-13 14:01:32,095 DeMo.train INFO: CMC curve, Rank-10 :85.2%
2025-06-13 14:01:32,095 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:01:48,650 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:01:48,650 DeMo.train INFO: Best mAP: 59.1%
2025-06-13 14:01:48,651 DeMo.train INFO: Best Rank-1: 83.7%
2025-06-13 14:01:48,651 DeMo.train INFO: Best Rank-5: 84.4%
2025-06-13 14:01:48,651 DeMo.train INFO: Best Rank-10: 85.2%
2025-06-13 14:01:48,651 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:01:58,412 DeMo.train INFO: Epoch[2] Iteration[10/65] Loss: 6.339, Acc: 0.502, Base Lr: 1.61e-04
2025-06-13 14:02:05,636 DeMo.train INFO: Epoch[2] Iteration[20/65] Loss: 6.180, Acc: 0.549, Base Lr: 1.61e-04
2025-06-13 14:02:12,825 DeMo.train INFO: Epoch[2] Iteration[30/65] Loss: 6.049, Acc: 0.579, Base Lr: 1.61e-04
2025-06-13 14:02:19,988 DeMo.train INFO: Epoch[2] Iteration[40/65] Loss: 5.885, Acc: 0.620, Base Lr: 1.61e-04
2025-06-13 14:02:27,220 DeMo.train INFO: Epoch[2] Iteration[50/65] Loss: 5.708, Acc: 0.681, Base Lr: 1.61e-04
2025-06-13 14:02:34,408 DeMo.train INFO: Epoch[2] Iteration[60/65] Loss: 5.557, Acc: 0.712, Base Lr: 1.61e-04
2025-06-13 14:02:36,000 DeMo.train INFO: Epoch 2 done. Time per batch: 0.764[s] Speed: 167.6[samples/s]
2025-06-13 14:02:36,011 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:02:36,012 DeMo.train INFO: Current is the ori feature testing!
2025-06-13 14:02:36,012 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.6412
CMC curve: Rank-1: 0.8397, Rank-5: 0.8542, Rank-10: 0.8665
=> Enter reranking
=> Results after reranking:
mAP: 0.6502
CMC curve: Rank-1: 0.8245, Rank-5: 0.8309, Rank-10: 0.8362
=> Improvement:
mAP improvement: 0.0089 (+1.39%)
Rank-1 improvement: -0.0152 (-1.81%)
tstpic False mxa
2025-06-13 14:04:35,535 DeMo.train INFO: Validation Results - Epoch: 2
2025-06-13 14:04:35,535 DeMo.train INFO: mAP: 65.0%
2025-06-13 14:04:35,536 DeMo.train INFO: CMC curve, Rank-1  :82.4%
2025-06-13 14:04:35,536 DeMo.train INFO: CMC curve, Rank-5  :83.1%
2025-06-13 14:04:35,536 DeMo.train INFO: CMC curve, Rank-10 :83.6%
2025-06-13 14:04:35,536 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:04:35,660 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:04:35,660 DeMo.train INFO: Current is the moe feature testing!
2025-06-13 14:04:35,660 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.6168
CMC curve: Rank-1: 0.8064, Rank-5: 0.8332, Rank-10: 0.8536
=> Enter reranking
=> Results after reranking:
mAP: 0.6224
CMC curve: Rank-1: 0.7936, Rank-5: 0.8047, Rank-10: 0.8157
=> Improvement:
mAP improvement: 0.0056 (+0.91%)
Rank-1 improvement: -0.0128 (-1.59%)
tstpic False mxa
2025-06-13 14:06:34,667 DeMo.train INFO: Validation Results - Epoch: 2
2025-06-13 14:06:34,668 DeMo.train INFO: mAP: 62.2%
2025-06-13 14:06:34,668 DeMo.train INFO: CMC curve, Rank-1  :79.4%
2025-06-13 14:06:34,668 DeMo.train INFO: CMC curve, Rank-5  :80.5%
2025-06-13 14:06:34,668 DeMo.train INFO: CMC curve, Rank-10 :81.6%
2025-06-13 14:06:34,668 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:06:35,933 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:06:35,933 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-13 14:06:35,933 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.6423
CMC curve: Rank-1: 0.8385, Rank-5: 0.8577, Rank-10: 0.8706
=> Enter reranking
=> Results after reranking:
mAP: 0.6511
CMC curve: Rank-1: 0.8268, Rank-5: 0.8367, Rank-10: 0.8437
=> Improvement:
mAP improvement: 0.0088 (+1.36%)
Rank-1 improvement: -0.0117 (-1.39%)
tstpic False mxa
2025-06-13 14:08:35,684 DeMo.train INFO: Validation Results - Epoch: 2
2025-06-13 14:08:35,684 DeMo.train INFO: mAP: 65.1%
2025-06-13 14:08:35,684 DeMo.train INFO: CMC curve, Rank-1  :82.7%
2025-06-13 14:08:35,685 DeMo.train INFO: CMC curve, Rank-5  :83.7%
2025-06-13 14:08:35,685 DeMo.train INFO: CMC curve, Rank-10 :84.4%
2025-06-13 14:08:35,685 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:08:46,072 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:08:46,072 DeMo.train INFO: Best mAP: 65.1%
2025-06-13 14:08:46,072 DeMo.train INFO: Best Rank-1: 82.7%
2025-06-13 14:08:46,073 DeMo.train INFO: Best Rank-5: 83.7%
2025-06-13 14:08:46,073 DeMo.train INFO: Best Rank-10: 84.4%
2025-06-13 14:08:46,073 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:08:55,564 DeMo.train INFO: Epoch[3] Iteration[10/65] Loss: 4.616, Acc: 0.771, Base Lr: 2.24e-04
2025-06-13 14:09:02,774 DeMo.train INFO: Epoch[3] Iteration[20/65] Loss: 4.486, Acc: 0.819, Base Lr: 2.24e-04
2025-06-13 14:09:09,970 DeMo.train INFO: Epoch[3] Iteration[30/65] Loss: 4.260, Acc: 0.832, Base Lr: 2.24e-04
2025-06-13 14:09:17,165 DeMo.train INFO: Epoch[3] Iteration[40/65] Loss: 4.110, Acc: 0.863, Base Lr: 2.24e-04
2025-06-13 14:09:24,363 DeMo.train INFO: Epoch[3] Iteration[50/65] Loss: 3.952, Acc: 0.875, Base Lr: 2.24e-04
2025-06-13 14:09:31,469 DeMo.train INFO: Epoch[3] Iteration[60/65] Loss: 3.854, Acc: 0.887, Base Lr: 2.24e-04
2025-06-13 14:09:33,782 DeMo.train INFO: Epoch 3 done. Time per batch: 0.757[s] Speed: 169.0[samples/s]
2025-06-13 14:09:33,795 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:09:33,796 DeMo.train INFO: Current is the ori feature testing!
2025-06-13 14:09:33,796 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.7370
CMC curve: Rank-1: 0.9020, Rank-5: 0.9178, Rank-10: 0.9265
=> Enter reranking
=> Results after reranking:
mAP: 0.7369
CMC curve: Rank-1: 0.8945, Rank-5: 0.9032, Rank-10: 0.9085
=> Improvement:
mAP improvement: -0.0001 (-0.02%)
Rank-1 improvement: -0.0076 (-0.84%)
tstpic False mxa
2025-06-13 14:11:31,908 DeMo.train INFO: Validation Results - Epoch: 3
2025-06-13 14:11:31,908 DeMo.train INFO: mAP: 73.7%
2025-06-13 14:11:31,908 DeMo.train INFO: CMC curve, Rank-1  :89.4%
2025-06-13 14:11:31,908 DeMo.train INFO: CMC curve, Rank-5  :90.3%
2025-06-13 14:11:31,908 DeMo.train INFO: CMC curve, Rank-10 :90.8%
2025-06-13 14:11:31,908 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:11:32,507 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:11:32,508 DeMo.train INFO: Current is the moe feature testing!
2025-06-13 14:11:32,508 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.7038
CMC curve: Rank-1: 0.8706, Rank-5: 0.8933, Rank-10: 0.9044
=> Enter reranking
=> Results after reranking:
mAP: 0.7082
CMC curve: Rank-1: 0.8764, Rank-5: 0.8851, Rank-10: 0.8945
=> Improvement:
mAP improvement: 0.0044 (+0.62%)
Rank-1 improvement: 0.0058 (+0.67%)
tstpic False mxa
2025-06-13 14:13:32,223 DeMo.train INFO: Validation Results - Epoch: 3
2025-06-13 14:13:32,223 DeMo.train INFO: mAP: 70.8%
2025-06-13 14:13:32,224 DeMo.train INFO: CMC curve, Rank-1  :87.6%
2025-06-13 14:13:32,224 DeMo.train INFO: CMC curve, Rank-5  :88.5%
2025-06-13 14:13:32,224 DeMo.train INFO: CMC curve, Rank-10 :89.4%
2025-06-13 14:13:32,224 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:13:32,238 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:13:32,238 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-13 14:13:32,239 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.7384
CMC curve: Rank-1: 0.9020, Rank-5: 0.9195, Rank-10: 0.9271
=> Enter reranking
=> Results after reranking:
mAP: 0.7384
CMC curve: Rank-1: 0.8962, Rank-5: 0.9055, Rank-10: 0.9085
=> Improvement:
mAP improvement: -0.0001 (-0.01%)
Rank-1 improvement: -0.0058 (-0.65%)
tstpic False mxa
2025-06-13 14:15:32,449 DeMo.train INFO: Validation Results - Epoch: 3
2025-06-13 14:15:32,449 DeMo.train INFO: mAP: 73.8%
2025-06-13 14:15:32,449 DeMo.train INFO: CMC curve, Rank-1  :89.6%
2025-06-13 14:15:32,449 DeMo.train INFO: CMC curve, Rank-5  :90.6%
2025-06-13 14:15:32,449 DeMo.train INFO: CMC curve, Rank-10 :90.8%
2025-06-13 14:15:32,449 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:15:41,962 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:15:41,962 DeMo.train INFO: Best mAP: 73.8%
2025-06-13 14:15:41,962 DeMo.train INFO: Best Rank-1: 89.6%
2025-06-13 14:15:41,962 DeMo.train INFO: Best Rank-5: 90.6%
2025-06-13 14:15:41,962 DeMo.train INFO: Best Rank-10: 90.8%
2025-06-13 14:15:41,962 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:15:51,039 DeMo.train INFO: Epoch[4] Iteration[10/65] Loss: 2.804, Acc: 0.932, Base Lr: 2.87e-04
2025-06-13 14:15:58,273 DeMo.train INFO: Epoch[4] Iteration[20/65] Loss: 2.523, Acc: 0.961, Base Lr: 2.87e-04
2025-06-13 14:16:05,498 DeMo.train INFO: Epoch[4] Iteration[30/65] Loss: 2.361, Acc: 0.973, Base Lr: 2.87e-04
2025-06-13 14:16:12,688 DeMo.train INFO: Epoch[4] Iteration[40/65] Loss: 2.268, Acc: 0.980, Base Lr: 2.87e-04
2025-06-13 14:16:19,823 DeMo.train INFO: Epoch[4] Iteration[50/65] Loss: 2.175, Acc: 0.981, Base Lr: 2.87e-04
2025-06-13 14:16:26,999 DeMo.train INFO: Epoch[4] Iteration[60/65] Loss: 2.098, Acc: 0.983, Base Lr: 2.87e-04
2025-06-13 14:16:28,588 DeMo.train INFO: Epoch 4 done. Time per batch: 0.752[s] Speed: 170.2[samples/s]
2025-06-13 14:16:28,602 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:16:28,602 DeMo.train INFO: Current is the ori feature testing!
2025-06-13 14:16:28,602 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.8200
CMC curve: Rank-1: 0.9452, Rank-5: 0.9557, Rank-10: 0.9586
=> Enter reranking
=> Results after reranking:
mAP: 0.8252
CMC curve: Rank-1: 0.9528, Rank-5: 0.9557, Rank-10: 0.9569
=> Improvement:
mAP improvement: 0.0052 (+0.63%)
Rank-1 improvement: 0.0076 (+0.80%)
tstpic False mxa
2025-06-13 14:18:26,606 DeMo.train INFO: Validation Results - Epoch: 4
2025-06-13 14:18:26,607 DeMo.train INFO: mAP: 82.5%
2025-06-13 14:18:26,607 DeMo.train INFO: CMC curve, Rank-1  :95.3%
2025-06-13 14:18:26,607 DeMo.train INFO: CMC curve, Rank-5  :95.6%
2025-06-13 14:18:26,607 DeMo.train INFO: CMC curve, Rank-10 :95.7%
2025-06-13 14:18:26,607 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:18:27,917 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:18:27,918 DeMo.train INFO: Current is the moe feature testing!
2025-06-13 14:18:27,918 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.7464
CMC curve: Rank-1: 0.9108, Rank-5: 0.9219, Rank-10: 0.9259
=> Enter reranking
=> Results after reranking:
mAP: 0.7549
CMC curve: Rank-1: 0.9155, Rank-5: 0.9248, Rank-10: 0.9300
=> Improvement:
mAP improvement: 0.0084 (+1.13%)
Rank-1 improvement: 0.0047 (+0.51%)
tstpic False mxa
2025-06-13 14:20:27,816 DeMo.train INFO: Validation Results - Epoch: 4
2025-06-13 14:20:27,816 DeMo.train INFO: mAP: 75.5%
2025-06-13 14:20:27,816 DeMo.train INFO: CMC curve, Rank-1  :91.5%
2025-06-13 14:20:27,816 DeMo.train INFO: CMC curve, Rank-5  :92.5%
2025-06-13 14:20:27,816 DeMo.train INFO: CMC curve, Rank-10 :93.0%
2025-06-13 14:20:27,816 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:20:28,446 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:20:28,447 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-13 14:20:28,447 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.8200
CMC curve: Rank-1: 0.9464, Rank-5: 0.9551, Rank-10: 0.9598
=> Enter reranking
=> Results after reranking:
mAP: 0.8253
CMC curve: Rank-1: 0.9539, Rank-5: 0.9557, Rank-10: 0.9569
=> Improvement:
mAP improvement: 0.0052 (+0.64%)
Rank-1 improvement: 0.0076 (+0.80%)
tstpic False mxa
2025-06-13 14:22:30,253 DeMo.train INFO: Validation Results - Epoch: 4
2025-06-13 14:22:30,253 DeMo.train INFO: mAP: 82.5%
2025-06-13 14:22:30,253 DeMo.train INFO: CMC curve, Rank-1  :95.4%
2025-06-13 14:22:30,253 DeMo.train INFO: CMC curve, Rank-5  :95.6%
2025-06-13 14:22:30,253 DeMo.train INFO: CMC curve, Rank-10 :95.7%
2025-06-13 14:22:30,253 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:22:42,178 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:22:42,179 DeMo.train INFO: Best mAP: 82.5%
2025-06-13 14:22:42,179 DeMo.train INFO: Best Rank-1: 95.4%
2025-06-13 14:22:42,179 DeMo.train INFO: Best Rank-5: 95.6%
2025-06-13 14:22:42,179 DeMo.train INFO: Best Rank-10: 95.7%
2025-06-13 14:22:42,179 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:22:51,595 DeMo.train INFO: Epoch[5] Iteration[10/65] Loss: 1.821, Acc: 0.998, Base Lr: 3.27e-04
2025-06-13 14:22:58,795 DeMo.train INFO: Epoch[5] Iteration[20/65] Loss: 1.671, Acc: 0.997, Base Lr: 3.27e-04
2025-06-13 14:23:05,983 DeMo.train INFO: Epoch[5] Iteration[30/65] Loss: 1.585, Acc: 0.991, Base Lr: 3.27e-04
2025-06-13 14:23:13,225 DeMo.train INFO: Epoch[5] Iteration[40/65] Loss: 1.505, Acc: 0.990, Base Lr: 3.27e-04
2025-06-13 14:23:20,377 DeMo.train INFO: Epoch[5] Iteration[50/65] Loss: 1.467, Acc: 0.990, Base Lr: 3.27e-04
2025-06-13 14:23:27,541 DeMo.train INFO: Epoch[5] Iteration[60/65] Loss: 1.427, Acc: 0.991, Base Lr: 3.27e-04
2025-06-13 14:23:29,875 DeMo.train INFO: Epoch 5 done. Time per batch: 0.757[s] Speed: 169.1[samples/s]
2025-06-13 14:23:29,887 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:23:29,887 DeMo.train INFO: Current is the ori feature testing!
2025-06-13 14:23:29,887 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.8302
CMC curve: Rank-1: 0.9516, Rank-5: 0.9563, Rank-10: 0.9633
=> Enter reranking
=> Results after reranking:
mAP: 0.8345
CMC curve: Rank-1: 0.9539, Rank-5: 0.9545, Rank-10: 0.9603
=> Improvement:
mAP improvement: 0.0043 (+0.52%)
Rank-1 improvement: 0.0023 (+0.25%)
tstpic False mxa
2025-06-13 14:25:27,897 DeMo.train INFO: Validation Results - Epoch: 5
2025-06-13 14:25:27,898 DeMo.train INFO: mAP: 83.5%
2025-06-13 14:25:27,898 DeMo.train INFO: CMC curve, Rank-1  :95.4%
2025-06-13 14:25:27,898 DeMo.train INFO: CMC curve, Rank-5  :95.5%
2025-06-13 14:25:27,898 DeMo.train INFO: CMC curve, Rank-10 :96.0%
2025-06-13 14:25:27,898 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:25:28,059 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:25:28,059 DeMo.train INFO: Current is the moe feature testing!
2025-06-13 14:25:28,059 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.7324
CMC curve: Rank-1: 0.8764, Rank-5: 0.8956, Rank-10: 0.9055
=> Enter reranking
=> Results after reranking:
mAP: 0.7373
CMC curve: Rank-1: 0.8694, Rank-5: 0.8787, Rank-10: 0.8875
=> Improvement:
mAP improvement: 0.0049 (+0.67%)
Rank-1 improvement: -0.0070 (-0.80%)
tstpic False mxa
2025-06-13 14:27:26,812 DeMo.train INFO: Validation Results - Epoch: 5
2025-06-13 14:27:26,813 DeMo.train INFO: mAP: 73.7%
2025-06-13 14:27:26,813 DeMo.train INFO: CMC curve, Rank-1  :86.9%
2025-06-13 14:27:26,813 DeMo.train INFO: CMC curve, Rank-5  :87.9%
2025-06-13 14:27:26,813 DeMo.train INFO: CMC curve, Rank-10 :88.7%
2025-06-13 14:27:26,813 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:27:26,831 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:27:26,832 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-13 14:27:26,832 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.8304
CMC curve: Rank-1: 0.9522, Rank-5: 0.9557, Rank-10: 0.9615
=> Enter reranking
=> Results after reranking:
mAP: 0.8345
CMC curve: Rank-1: 0.9545, Rank-5: 0.9545, Rank-10: 0.9586
=> Improvement:
mAP improvement: 0.0042 (+0.50%)
Rank-1 improvement: 0.0023 (+0.24%)
tstpic False mxa
2025-06-13 14:29:26,826 DeMo.train INFO: Validation Results - Epoch: 5
2025-06-13 14:29:26,826 DeMo.train INFO: mAP: 83.5%
2025-06-13 14:29:26,826 DeMo.train INFO: CMC curve, Rank-1  :95.5%
2025-06-13 14:29:26,826 DeMo.train INFO: CMC curve, Rank-5  :95.5%
2025-06-13 14:29:26,826 DeMo.train INFO: CMC curve, Rank-10 :95.9%
2025-06-13 14:29:26,826 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:29:36,509 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:29:36,509 DeMo.train INFO: Best mAP: 83.5%
2025-06-13 14:29:36,509 DeMo.train INFO: Best Rank-1: 95.5%
2025-06-13 14:29:36,509 DeMo.train INFO: Best Rank-5: 95.5%
2025-06-13 14:29:36,510 DeMo.train INFO: Best Rank-10: 95.9%
2025-06-13 14:29:36,510 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:29:46,111 DeMo.train INFO: Epoch[6] Iteration[10/65] Loss: 1.296, Acc: 0.941, Base Lr: 3.17e-04
2025-06-13 14:29:53,354 DeMo.train INFO: Epoch[6] Iteration[20/65] Loss: 1.281, Acc: 0.959, Base Lr: 3.17e-04
2025-06-13 14:30:00,511 DeMo.train INFO: Epoch[6] Iteration[30/65] Loss: 1.246, Acc: 0.972, Base Lr: 3.17e-04
2025-06-13 14:30:07,650 DeMo.train INFO: Epoch[6] Iteration[40/65] Loss: 1.220, Acc: 0.979, Base Lr: 3.17e-04
2025-06-13 14:30:14,841 DeMo.train INFO: Epoch[6] Iteration[50/65] Loss: 1.193, Acc: 0.983, Base Lr: 3.17e-04
2025-06-13 14:30:22,223 DeMo.train INFO: Epoch[6] Iteration[60/65] Loss: 1.172, Acc: 0.986, Base Lr: 3.17e-04
2025-06-13 14:30:24,532 DeMo.train INFO: Epoch 6 done. Time per batch: 0.762[s] Speed: 167.9[samples/s]
2025-06-13 14:30:24,545 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:30:24,545 DeMo.train INFO: Current is the ori feature testing!
2025-06-13 14:30:24,545 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.8122
CMC curve: Rank-1: 0.9551, Rank-5: 0.9627, Rank-10: 0.9656
=> Enter reranking
=> Results after reranking:
mAP: 0.8168
CMC curve: Rank-1: 0.9539, Rank-5: 0.9592, Rank-10: 0.9598
=> Improvement:
mAP improvement: 0.0046 (+0.56%)
Rank-1 improvement: -0.0012 (-0.12%)
tstpic False mxa
2025-06-13 14:32:23,229 DeMo.train INFO: Validation Results - Epoch: 6
2025-06-13 14:32:23,231 DeMo.train INFO: mAP: 81.7%
2025-06-13 14:32:23,231 DeMo.train INFO: CMC curve, Rank-1  :95.4%
2025-06-13 14:32:23,231 DeMo.train INFO: CMC curve, Rank-5  :95.9%
2025-06-13 14:32:23,231 DeMo.train INFO: CMC curve, Rank-10 :96.0%
2025-06-13 14:32:23,231 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:32:23,355 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:32:23,355 DeMo.train INFO: Current is the moe feature testing!
2025-06-13 14:32:23,355 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.7301
CMC curve: Rank-1: 0.8828, Rank-5: 0.8974, Rank-10: 0.9073
=> Enter reranking
=> Results after reranking:
mAP: 0.7385
CMC curve: Rank-1: 0.8945, Rank-5: 0.9020, Rank-10: 0.9073
=> Improvement:
mAP improvement: 0.0084 (+1.15%)
Rank-1 improvement: 0.0117 (+1.32%)
tstpic False mxa
2025-06-13 14:34:23,322 DeMo.train INFO: Validation Results - Epoch: 6
2025-06-13 14:34:23,322 DeMo.train INFO: mAP: 73.8%
2025-06-13 14:34:23,322 DeMo.train INFO: CMC curve, Rank-1  :89.4%
2025-06-13 14:34:23,322 DeMo.train INFO: CMC curve, Rank-5  :90.2%
2025-06-13 14:34:23,322 DeMo.train INFO: CMC curve, Rank-10 :90.7%
2025-06-13 14:34:23,322 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:34:23,618 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:34:23,618 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-13 14:34:23,618 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.8124
CMC curve: Rank-1: 0.9551, Rank-5: 0.9621, Rank-10: 0.9650
=> Enter reranking
=> Results after reranking:
mAP: 0.8168
CMC curve: Rank-1: 0.9545, Rank-5: 0.9580, Rank-10: 0.9586
=> Improvement:
mAP improvement: 0.0045 (+0.55%)
Rank-1 improvement: -0.0006 (-0.06%)
tstpic False mxa
2025-06-13 14:36:23,714 DeMo.train INFO: Validation Results - Epoch: 6
2025-06-13 14:36:23,714 DeMo.train INFO: mAP: 81.7%
2025-06-13 14:36:23,714 DeMo.train INFO: CMC curve, Rank-1  :95.5%
2025-06-13 14:36:23,714 DeMo.train INFO: CMC curve, Rank-5  :95.8%
2025-06-13 14:36:23,714 DeMo.train INFO: CMC curve, Rank-10 :95.9%
2025-06-13 14:36:23,714 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:36:24,823 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:36:24,823 DeMo.train INFO: Best mAP: 83.5%
2025-06-13 14:36:24,823 DeMo.train INFO: Best Rank-1: 95.5%
2025-06-13 14:36:24,823 DeMo.train INFO: Best Rank-5: 95.5%
2025-06-13 14:36:24,823 DeMo.train INFO: Best Rank-10: 95.9%
2025-06-13 14:36:24,823 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:36:34,531 DeMo.train INFO: Epoch[7] Iteration[10/65] Loss: 1.010, Acc: 0.990, Base Lr: 3.05e-04
2025-06-13 14:36:41,691 DeMo.train INFO: Epoch[7] Iteration[20/65] Loss: 1.012, Acc: 0.994, Base Lr: 3.05e-04
2025-06-13 14:36:48,864 DeMo.train INFO: Epoch[7] Iteration[30/65] Loss: 1.009, Acc: 0.996, Base Lr: 3.05e-04
2025-06-13 14:36:56,074 DeMo.train INFO: Epoch[7] Iteration[40/65] Loss: 0.993, Acc: 0.997, Base Lr: 3.05e-04
2025-06-13 14:37:03,276 DeMo.train INFO: Epoch[7] Iteration[50/65] Loss: 0.982, Acc: 0.998, Base Lr: 3.05e-04
2025-06-13 14:37:10,469 DeMo.train INFO: Epoch[7] Iteration[60/65] Loss: 0.972, Acc: 0.998, Base Lr: 3.05e-04
2025-06-13 14:37:12,798 DeMo.train INFO: Epoch 7 done. Time per batch: 0.762[s] Speed: 168.1[samples/s]
2025-06-13 14:37:12,809 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:37:12,809 DeMo.train INFO: Current is the ori feature testing!
2025-06-13 14:37:12,809 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.8396
CMC curve: Rank-1: 0.9586, Rank-5: 0.9644, Rank-10: 0.9673
=> Enter reranking
=> Results after reranking:
mAP: 0.8422
CMC curve: Rank-1: 0.9586, Rank-5: 0.9615, Rank-10: 0.9633
=> Improvement:
mAP improvement: 0.0026 (+0.31%)
Rank-1 improvement: 0.0000 (+0.00%)
tstpic False mxa
2025-06-13 14:39:11,559 DeMo.train INFO: Validation Results - Epoch: 7
2025-06-13 14:39:11,560 DeMo.train INFO: mAP: 84.2%
2025-06-13 14:39:11,560 DeMo.train INFO: CMC curve, Rank-1  :95.9%
2025-06-13 14:39:11,560 DeMo.train INFO: CMC curve, Rank-5  :96.2%
2025-06-13 14:39:11,560 DeMo.train INFO: CMC curve, Rank-10 :96.3%
2025-06-13 14:39:11,560 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:39:11,686 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:39:11,687 DeMo.train INFO: Current is the moe feature testing!
2025-06-13 14:39:11,687 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.7541
CMC curve: Rank-1: 0.8875, Rank-5: 0.8997, Rank-10: 0.9067
=> Enter reranking
=> Results after reranking:
mAP: 0.7576
CMC curve: Rank-1: 0.8840, Rank-5: 0.8892, Rank-10: 0.8968
=> Improvement:
mAP improvement: 0.0035 (+0.46%)
Rank-1 improvement: -0.0035 (-0.39%)
tstpic False mxa
2025-06-13 14:41:10,216 DeMo.train INFO: Validation Results - Epoch: 7
2025-06-13 14:41:10,216 DeMo.train INFO: mAP: 75.8%
2025-06-13 14:41:10,216 DeMo.train INFO: CMC curve, Rank-1  :88.4%
2025-06-13 14:41:10,217 DeMo.train INFO: CMC curve, Rank-5  :88.9%
2025-06-13 14:41:10,217 DeMo.train INFO: CMC curve, Rank-10 :89.7%
2025-06-13 14:41:10,217 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:41:10,232 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:41:10,232 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-13 14:41:10,233 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
