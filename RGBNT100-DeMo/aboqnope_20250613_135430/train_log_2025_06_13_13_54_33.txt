2025-06-13 13:54:33,505 DeMo INFO: Saving model in the path :./RGBNT100-DeMo/aboqnope_20250613_135430
2025-06-13 13:54:33,505 DeMo INFO: Namespace(config_file='configs/RGBNT100/DeMo.yml', fea_cft=0, local_rank=0, opts=['OUTPUT_DIR', './RGBNT100-DeMo/aboqnope_20250613_135430'])
2025-06-13 13:54:33,505 DeMo INFO: Loaded configuration file configs/RGBNT100/DeMo.yml
2025-06-13 13:54:33,505 DeMo INFO: 
MODEL:
  TRANSFORMER_TYPE: 'ViT-B-16'
  STRIDE_SIZE: [ 16, 16 ]
  SIE_CAMERA: True
  DIRECT: 0
  SIE_COE: 1.0
  ID_LOSS_WEIGHT: 0.25
  TRIPLET_LOSS_WEIGHT: 1.0
  GLOBAL_LOCAL: True
  HDM: True
  ATM: False
  HEAD: 8
  FROZEN: False
  CENGJIFUSION: False
  NEWDEFORM: True

INPUT:
  SIZE_TRAIN: [ 128, 256 ]
  SIZE_TEST: [ 128, 256 ]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.5 # random erasing
  PADDING: 10

DATALOADER:
  SAMPLER: 'softmax_triplet'
  NUM_INSTANCE: 16
  NUM_WORKERS: 14

DATASETS:
  NAMES: ('RGBNT100')
  ROOT_DIR: '..'

SOLVER:
  BASE_LR: 0.00035
  WARMUP_ITERS: 5
  MAX_EPOCHS: 30
  OPTIMIZER_NAME: 'Adam'
  GAMMA: 0.1
  IMS_PER_BATCH: 128
  EVAL_PERIOD: 1
  SEED: 1111

TEST:
  IMS_PER_BATCH: 128
  RE_RANKING: 'no'
  WEIGHT: ''
  NECK_FEAT: 'before'
  FEAT_NORM: 'yes'
  MISS: "nothing"

OUTPUT_DIR: '..'




2025-06-13 13:54:33,506 DeMo INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 16
  NUM_WORKERS: 14
  SAMPLER: softmax_triplet
DATASETS:
  NAMES: RGBNT100
  ROOT_DIR: ..
INPUT:
  PADDING: 10
  PIXEL_MEAN: [0.5, 0.5, 0.5]
  PIXEL_STD: [0.5, 0.5, 0.5]
  PROB: 0.5
  RE_PROB: 0.5
  SIZE_TEST: [128, 256]
  SIZE_TRAIN: [128, 256]
MODEL:
  ADAPTER: False
  ATM: False
  ATT_DROP_RATE: 0.0
  CENGJIFUSION: False
  DEVICE: cuda
  DEVICE_ID: 0
  DIRECT: 0
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FROZEN: False
  GLOBAL_LOCAL: True
  HDM: True
  HEAD: 8
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 0.25
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  METRIC_LOSS_TYPE: triplet
  NAME: DeMo
  NECK: bnneck
  NEWDEFORM: True
  NO_MARGIN: True
  PRETRAIN_PATH_T: /path/to/your/vitb_16_224_21k.pth
  PROMPT: False
  SIE_CAMERA: True
  SIE_COE: 1.0
  SIE_VIEW: False
  STRIDE_SIZE: [16, 16]
  TRANSFORMER_TYPE: ViT-B-16
  TRIPLET_LOSS_WEIGHT: 1.0
OUTPUT_DIR: ./RGBNT100-DeMo/aboqnope_20250613_135430
SOLVER:
  BASE_LR: 0.00035
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 60
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.1
  IMS_PER_BATCH: 128
  LARGE_FC_LR: False
  LOG_PERIOD: 10
  MARGIN: 0.3
  MAX_EPOCHS: 30
  MOMENTUM: 0.9
  OPTIMIZER_NAME: Adam
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1111
  STEPS: (40, 70)
  WARMUP_FACTOR: 0.01
  WARMUP_ITERS: 5
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
TEST:
  FEAT: 0
  FEAT_NORM: yes
  IMS_PER_BATCH: 128
  MISS: nothing
  NECK_FEAT: before
  RE_RANKING: no
  WEIGHT: 
2025-06-13 13:54:38,105 DeMo INFO: combineway: adaptiveboqdeformablation
2025-06-13 13:54:39,245 DeMo INFO: DeMo(
  (BACKBONE): build_transformer(
    (base): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (pool): AdaptiveAvgPool1d(output_size=1)
  (rgb_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (nir_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (tir_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (generalFusion): GeneralFusion(
    (modalityboqablation): TrulyModalitySpecificBoQAblation(
      (rgb_boq): AdaptiveBoQAblation(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncodingAblation()
        (boqs): ModuleList(
          (0): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=64, out_features=57, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=57, out_features=51, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=51, out_features=44, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusionAblation(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=216, out_features=1, bias=True)
        )
      )
      (ni_boq): AdaptiveBoQAblation(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncodingAblation()
        (boqs): ModuleList(
          (0): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=64, out_features=57, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=57, out_features=51, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=51, out_features=44, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusionAblation(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=216, out_features=1, bias=True)
        )
      )
      (ti_boq): AdaptiveBoQAblation(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncodingAblation()
        (boqs): ModuleList(
          (0): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=64, out_features=57, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=57, out_features=51, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=51, out_features=44, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusionAblation(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=216, out_features=1, bias=True)
        )
      )
      (rgb_ni_boq): AdaptiveBoQAblation(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncodingAblation()
        (boqs): ModuleList(
          (0): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=76, out_features=68, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=68, out_features=60, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=60, out_features=53, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusionAblation(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=257, out_features=1, bias=True)
        )
      )
      (rgb_ti_boq): AdaptiveBoQAblation(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncodingAblation()
        (boqs): ModuleList(
          (0): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=76, out_features=68, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=68, out_features=60, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=60, out_features=53, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusionAblation(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=257, out_features=1, bias=True)
        )
      )
      (ni_ti_boq): AdaptiveBoQAblation(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncodingAblation()
        (boqs): ModuleList(
          (0): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=76, out_features=68, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=68, out_features=60, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=60, out_features=53, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusionAblation(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=257, out_features=1, bias=True)
        )
      )
      (rgb_ni_ti_boq): AdaptiveBoQAblation(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncodingAblation()
        (boqs): ModuleList(
          (0): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=96, out_features=86, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=86, out_features=76, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlockAblation(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=76, out_features=67, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusionAblation(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=325, out_features=1, bias=True)
        )
      )
    )
    (deformselectablation): DAttentionEnhancedAblation(
      (modal_gate): Sequential(
        (0): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))
        (3): Sigmoid()
      )
      (conv_offset): Sequential(
        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), groups=512)
        (3): GELU(approximate='none')
        (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (conv_offset_coarse): Sequential(
        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(512, 512, kernel_size=(6, 6), stride=(2, 2), padding=(1, 1), groups=512)
        (3): GELU(approximate='none')
        (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (conv_offset_fine): Sequential(
        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2), groups=512)
        (3): GELU(approximate='none')
        (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (proj_q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attn_drop): Dropout(p=0.0, inplace=False)
    )
  )
  (classifier_moe): Linear(in_features=3584, out_features=50, bias=False)
  (bottleneck_moe): BatchNorm1d(3584, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier_r): Linear(in_features=512, out_features=50, bias=False)
  (bottleneck_r): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier_n): Linear(in_features=512, out_features=50, bias=False)
  (bottleneck_n): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier_t): Linear(in_features=512, out_features=50, bias=False)
  (bottleneck_t): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
2025-06-13 13:54:39,248 DeMo INFO: number of parameters:247.590725
2025-06-13 13:54:39,482 DeMo.train INFO: start training
2025-06-13 13:54:53,248 DeMo.train INFO: Epoch[1] Iteration[10/65] Loss: 9.632, Acc: 0.111, Base Lr: 9.80e-05
2025-06-13 13:55:00,430 DeMo.train INFO: Epoch[1] Iteration[20/65] Loss: 8.351, Acc: 0.158, Base Lr: 9.80e-05
2025-06-13 13:55:07,604 DeMo.train INFO: Epoch[1] Iteration[30/65] Loss: 7.856, Acc: 0.209, Base Lr: 9.80e-05
2025-06-13 13:55:14,944 DeMo.train INFO: Epoch[1] Iteration[40/65] Loss: 7.580, Acc: 0.265, Base Lr: 9.80e-05
2025-06-13 13:55:22,129 DeMo.train INFO: Epoch[1] Iteration[50/65] Loss: 7.396, Acc: 0.319, Base Lr: 9.80e-05
2025-06-13 13:55:29,498 DeMo.train INFO: Epoch[1] Iteration[60/65] Loss: 7.256, Acc: 0.368, Base Lr: 9.80e-05
2025-06-13 13:55:32,037 DeMo.train INFO: Epoch 1 done. Time per batch: 0.832[s] Speed: 153.8[samples/s]
2025-06-13 13:55:32,042 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 13:55:32,042 DeMo.train INFO: Current is the ori feature testing!
2025-06-13 13:55:32,042 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 13:57:32,454 DeMo.train INFO: Validation Results - Epoch: 1
2025-06-13 13:57:32,455 DeMo.train INFO: mAP: 57.8%
2025-06-13 13:57:32,455 DeMo.train INFO: CMC curve, Rank-1  :83.3%
2025-06-13 13:57:32,455 DeMo.train INFO: CMC curve, Rank-5  :84.6%
2025-06-13 13:57:32,455 DeMo.train INFO: CMC curve, Rank-10 :85.5%
2025-06-13 13:57:32,455 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 13:57:32,588 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 13:57:32,588 DeMo.train INFO: Current is the moe feature testing!
2025-06-13 13:57:32,588 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 13:59:33,147 DeMo.train INFO: Validation Results - Epoch: 1
2025-06-13 13:59:33,148 DeMo.train INFO: mAP: 57.0%
2025-06-13 13:59:33,148 DeMo.train INFO: CMC curve, Rank-1  :81.6%
2025-06-13 13:59:33,148 DeMo.train INFO: CMC curve, Rank-5  :83.0%
2025-06-13 13:59:33,148 DeMo.train INFO: CMC curve, Rank-10 :84.0%
2025-06-13 13:59:33,148 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 13:59:33,164 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 13:59:33,164 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-13 13:59:33,164 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:01:32,094 DeMo.train INFO: Validation Results - Epoch: 1
2025-06-13 14:01:32,094 DeMo.train INFO: mAP: 59.1%
2025-06-13 14:01:32,094 DeMo.train INFO: CMC curve, Rank-1  :83.7%
2025-06-13 14:01:32,095 DeMo.train INFO: CMC curve, Rank-5  :84.4%
2025-06-13 14:01:32,095 DeMo.train INFO: CMC curve, Rank-10 :85.2%
2025-06-13 14:01:32,095 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:01:48,650 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:01:48,650 DeMo.train INFO: Best mAP: 59.1%
2025-06-13 14:01:48,651 DeMo.train INFO: Best Rank-1: 83.7%
2025-06-13 14:01:48,651 DeMo.train INFO: Best Rank-5: 84.4%
2025-06-13 14:01:48,651 DeMo.train INFO: Best Rank-10: 85.2%
2025-06-13 14:01:48,651 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:01:58,412 DeMo.train INFO: Epoch[2] Iteration[10/65] Loss: 6.339, Acc: 0.502, Base Lr: 1.61e-04
2025-06-13 14:02:05,636 DeMo.train INFO: Epoch[2] Iteration[20/65] Loss: 6.180, Acc: 0.549, Base Lr: 1.61e-04
2025-06-13 14:02:12,825 DeMo.train INFO: Epoch[2] Iteration[30/65] Loss: 6.049, Acc: 0.579, Base Lr: 1.61e-04
2025-06-13 14:02:19,988 DeMo.train INFO: Epoch[2] Iteration[40/65] Loss: 5.885, Acc: 0.620, Base Lr: 1.61e-04
2025-06-13 14:02:27,220 DeMo.train INFO: Epoch[2] Iteration[50/65] Loss: 5.708, Acc: 0.681, Base Lr: 1.61e-04
2025-06-13 14:02:34,408 DeMo.train INFO: Epoch[2] Iteration[60/65] Loss: 5.557, Acc: 0.712, Base Lr: 1.61e-04
2025-06-13 14:02:36,000 DeMo.train INFO: Epoch 2 done. Time per batch: 0.764[s] Speed: 167.6[samples/s]
2025-06-13 14:02:36,011 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:02:36,012 DeMo.train INFO: Current is the ori feature testing!
2025-06-13 14:02:36,012 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:04:35,535 DeMo.train INFO: Validation Results - Epoch: 2
2025-06-13 14:04:35,535 DeMo.train INFO: mAP: 65.0%
2025-06-13 14:04:35,536 DeMo.train INFO: CMC curve, Rank-1  :82.4%
2025-06-13 14:04:35,536 DeMo.train INFO: CMC curve, Rank-5  :83.1%
2025-06-13 14:04:35,536 DeMo.train INFO: CMC curve, Rank-10 :83.6%
2025-06-13 14:04:35,536 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:04:35,660 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:04:35,660 DeMo.train INFO: Current is the moe feature testing!
2025-06-13 14:04:35,660 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:06:34,667 DeMo.train INFO: Validation Results - Epoch: 2
2025-06-13 14:06:34,668 DeMo.train INFO: mAP: 62.2%
2025-06-13 14:06:34,668 DeMo.train INFO: CMC curve, Rank-1  :79.4%
2025-06-13 14:06:34,668 DeMo.train INFO: CMC curve, Rank-5  :80.5%
2025-06-13 14:06:34,668 DeMo.train INFO: CMC curve, Rank-10 :81.6%
2025-06-13 14:06:34,668 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:06:35,933 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:06:35,933 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-13 14:06:35,933 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:08:35,684 DeMo.train INFO: Validation Results - Epoch: 2
2025-06-13 14:08:35,684 DeMo.train INFO: mAP: 65.1%
2025-06-13 14:08:35,684 DeMo.train INFO: CMC curve, Rank-1  :82.7%
2025-06-13 14:08:35,685 DeMo.train INFO: CMC curve, Rank-5  :83.7%
2025-06-13 14:08:35,685 DeMo.train INFO: CMC curve, Rank-10 :84.4%
2025-06-13 14:08:35,685 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:08:46,072 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:08:46,072 DeMo.train INFO: Best mAP: 65.1%
2025-06-13 14:08:46,072 DeMo.train INFO: Best Rank-1: 82.7%
2025-06-13 14:08:46,073 DeMo.train INFO: Best Rank-5: 83.7%
2025-06-13 14:08:46,073 DeMo.train INFO: Best Rank-10: 84.4%
2025-06-13 14:08:46,073 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:08:55,564 DeMo.train INFO: Epoch[3] Iteration[10/65] Loss: 4.616, Acc: 0.771, Base Lr: 2.24e-04
2025-06-13 14:09:02,774 DeMo.train INFO: Epoch[3] Iteration[20/65] Loss: 4.486, Acc: 0.819, Base Lr: 2.24e-04
2025-06-13 14:09:09,970 DeMo.train INFO: Epoch[3] Iteration[30/65] Loss: 4.260, Acc: 0.832, Base Lr: 2.24e-04
2025-06-13 14:09:17,165 DeMo.train INFO: Epoch[3] Iteration[40/65] Loss: 4.110, Acc: 0.863, Base Lr: 2.24e-04
2025-06-13 14:09:24,363 DeMo.train INFO: Epoch[3] Iteration[50/65] Loss: 3.952, Acc: 0.875, Base Lr: 2.24e-04
2025-06-13 14:09:31,469 DeMo.train INFO: Epoch[3] Iteration[60/65] Loss: 3.854, Acc: 0.887, Base Lr: 2.24e-04
2025-06-13 14:09:33,782 DeMo.train INFO: Epoch 3 done. Time per batch: 0.757[s] Speed: 169.0[samples/s]
2025-06-13 14:09:33,795 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:09:33,796 DeMo.train INFO: Current is the ori feature testing!
2025-06-13 14:09:33,796 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:11:31,908 DeMo.train INFO: Validation Results - Epoch: 3
2025-06-13 14:11:31,908 DeMo.train INFO: mAP: 73.7%
2025-06-13 14:11:31,908 DeMo.train INFO: CMC curve, Rank-1  :89.4%
2025-06-13 14:11:31,908 DeMo.train INFO: CMC curve, Rank-5  :90.3%
2025-06-13 14:11:31,908 DeMo.train INFO: CMC curve, Rank-10 :90.8%
2025-06-13 14:11:31,908 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:11:32,507 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:11:32,508 DeMo.train INFO: Current is the moe feature testing!
2025-06-13 14:11:32,508 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:13:32,223 DeMo.train INFO: Validation Results - Epoch: 3
2025-06-13 14:13:32,223 DeMo.train INFO: mAP: 70.8%
2025-06-13 14:13:32,224 DeMo.train INFO: CMC curve, Rank-1  :87.6%
2025-06-13 14:13:32,224 DeMo.train INFO: CMC curve, Rank-5  :88.5%
2025-06-13 14:13:32,224 DeMo.train INFO: CMC curve, Rank-10 :89.4%
2025-06-13 14:13:32,224 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:13:32,238 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:13:32,238 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-13 14:13:32,239 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:15:32,449 DeMo.train INFO: Validation Results - Epoch: 3
2025-06-13 14:15:32,449 DeMo.train INFO: mAP: 73.8%
2025-06-13 14:15:32,449 DeMo.train INFO: CMC curve, Rank-1  :89.6%
2025-06-13 14:15:32,449 DeMo.train INFO: CMC curve, Rank-5  :90.6%
2025-06-13 14:15:32,449 DeMo.train INFO: CMC curve, Rank-10 :90.8%
2025-06-13 14:15:32,449 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:15:41,962 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:15:41,962 DeMo.train INFO: Best mAP: 73.8%
2025-06-13 14:15:41,962 DeMo.train INFO: Best Rank-1: 89.6%
2025-06-13 14:15:41,962 DeMo.train INFO: Best Rank-5: 90.6%
2025-06-13 14:15:41,962 DeMo.train INFO: Best Rank-10: 90.8%
2025-06-13 14:15:41,962 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:15:51,039 DeMo.train INFO: Epoch[4] Iteration[10/65] Loss: 2.804, Acc: 0.932, Base Lr: 2.87e-04
2025-06-13 14:15:58,273 DeMo.train INFO: Epoch[4] Iteration[20/65] Loss: 2.523, Acc: 0.961, Base Lr: 2.87e-04
2025-06-13 14:16:05,498 DeMo.train INFO: Epoch[4] Iteration[30/65] Loss: 2.361, Acc: 0.973, Base Lr: 2.87e-04
2025-06-13 14:16:12,688 DeMo.train INFO: Epoch[4] Iteration[40/65] Loss: 2.268, Acc: 0.980, Base Lr: 2.87e-04
2025-06-13 14:16:19,823 DeMo.train INFO: Epoch[4] Iteration[50/65] Loss: 2.175, Acc: 0.981, Base Lr: 2.87e-04
2025-06-13 14:16:26,999 DeMo.train INFO: Epoch[4] Iteration[60/65] Loss: 2.098, Acc: 0.983, Base Lr: 2.87e-04
2025-06-13 14:16:28,588 DeMo.train INFO: Epoch 4 done. Time per batch: 0.752[s] Speed: 170.2[samples/s]
2025-06-13 14:16:28,602 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:16:28,602 DeMo.train INFO: Current is the ori feature testing!
2025-06-13 14:16:28,602 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:18:26,606 DeMo.train INFO: Validation Results - Epoch: 4
2025-06-13 14:18:26,607 DeMo.train INFO: mAP: 82.5%
2025-06-13 14:18:26,607 DeMo.train INFO: CMC curve, Rank-1  :95.3%
2025-06-13 14:18:26,607 DeMo.train INFO: CMC curve, Rank-5  :95.6%
2025-06-13 14:18:26,607 DeMo.train INFO: CMC curve, Rank-10 :95.7%
2025-06-13 14:18:26,607 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:18:27,917 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:18:27,918 DeMo.train INFO: Current is the moe feature testing!
2025-06-13 14:18:27,918 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:20:27,816 DeMo.train INFO: Validation Results - Epoch: 4
2025-06-13 14:20:27,816 DeMo.train INFO: mAP: 75.5%
2025-06-13 14:20:27,816 DeMo.train INFO: CMC curve, Rank-1  :91.5%
2025-06-13 14:20:27,816 DeMo.train INFO: CMC curve, Rank-5  :92.5%
2025-06-13 14:20:27,816 DeMo.train INFO: CMC curve, Rank-10 :93.0%
2025-06-13 14:20:27,816 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:20:28,446 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:20:28,447 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-13 14:20:28,447 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:22:30,253 DeMo.train INFO: Validation Results - Epoch: 4
2025-06-13 14:22:30,253 DeMo.train INFO: mAP: 82.5%
2025-06-13 14:22:30,253 DeMo.train INFO: CMC curve, Rank-1  :95.4%
2025-06-13 14:22:30,253 DeMo.train INFO: CMC curve, Rank-5  :95.6%
2025-06-13 14:22:30,253 DeMo.train INFO: CMC curve, Rank-10 :95.7%
2025-06-13 14:22:30,253 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:22:42,178 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:22:42,179 DeMo.train INFO: Best mAP: 82.5%
2025-06-13 14:22:42,179 DeMo.train INFO: Best Rank-1: 95.4%
2025-06-13 14:22:42,179 DeMo.train INFO: Best Rank-5: 95.6%
2025-06-13 14:22:42,179 DeMo.train INFO: Best Rank-10: 95.7%
2025-06-13 14:22:42,179 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:22:51,595 DeMo.train INFO: Epoch[5] Iteration[10/65] Loss: 1.821, Acc: 0.998, Base Lr: 3.27e-04
2025-06-13 14:22:58,795 DeMo.train INFO: Epoch[5] Iteration[20/65] Loss: 1.671, Acc: 0.997, Base Lr: 3.27e-04
2025-06-13 14:23:05,983 DeMo.train INFO: Epoch[5] Iteration[30/65] Loss: 1.585, Acc: 0.991, Base Lr: 3.27e-04
2025-06-13 14:23:13,225 DeMo.train INFO: Epoch[5] Iteration[40/65] Loss: 1.505, Acc: 0.990, Base Lr: 3.27e-04
2025-06-13 14:23:20,377 DeMo.train INFO: Epoch[5] Iteration[50/65] Loss: 1.467, Acc: 0.990, Base Lr: 3.27e-04
2025-06-13 14:23:27,541 DeMo.train INFO: Epoch[5] Iteration[60/65] Loss: 1.427, Acc: 0.991, Base Lr: 3.27e-04
2025-06-13 14:23:29,875 DeMo.train INFO: Epoch 5 done. Time per batch: 0.757[s] Speed: 169.1[samples/s]
2025-06-13 14:23:29,887 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:23:29,887 DeMo.train INFO: Current is the ori feature testing!
2025-06-13 14:23:29,887 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:25:27,897 DeMo.train INFO: Validation Results - Epoch: 5
2025-06-13 14:25:27,898 DeMo.train INFO: mAP: 83.5%
2025-06-13 14:25:27,898 DeMo.train INFO: CMC curve, Rank-1  :95.4%
2025-06-13 14:25:27,898 DeMo.train INFO: CMC curve, Rank-5  :95.5%
2025-06-13 14:25:27,898 DeMo.train INFO: CMC curve, Rank-10 :96.0%
2025-06-13 14:25:27,898 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:25:28,059 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:25:28,059 DeMo.train INFO: Current is the moe feature testing!
2025-06-13 14:25:28,059 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:27:26,812 DeMo.train INFO: Validation Results - Epoch: 5
2025-06-13 14:27:26,813 DeMo.train INFO: mAP: 73.7%
2025-06-13 14:27:26,813 DeMo.train INFO: CMC curve, Rank-1  :86.9%
2025-06-13 14:27:26,813 DeMo.train INFO: CMC curve, Rank-5  :87.9%
2025-06-13 14:27:26,813 DeMo.train INFO: CMC curve, Rank-10 :88.7%
2025-06-13 14:27:26,813 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:27:26,831 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:27:26,832 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-13 14:27:26,832 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:29:26,826 DeMo.train INFO: Validation Results - Epoch: 5
2025-06-13 14:29:26,826 DeMo.train INFO: mAP: 83.5%
2025-06-13 14:29:26,826 DeMo.train INFO: CMC curve, Rank-1  :95.5%
2025-06-13 14:29:26,826 DeMo.train INFO: CMC curve, Rank-5  :95.5%
2025-06-13 14:29:26,826 DeMo.train INFO: CMC curve, Rank-10 :95.9%
2025-06-13 14:29:26,826 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:29:36,509 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:29:36,509 DeMo.train INFO: Best mAP: 83.5%
2025-06-13 14:29:36,509 DeMo.train INFO: Best Rank-1: 95.5%
2025-06-13 14:29:36,509 DeMo.train INFO: Best Rank-5: 95.5%
2025-06-13 14:29:36,510 DeMo.train INFO: Best Rank-10: 95.9%
2025-06-13 14:29:36,510 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:29:46,111 DeMo.train INFO: Epoch[6] Iteration[10/65] Loss: 1.296, Acc: 0.941, Base Lr: 3.17e-04
2025-06-13 14:29:53,354 DeMo.train INFO: Epoch[6] Iteration[20/65] Loss: 1.281, Acc: 0.959, Base Lr: 3.17e-04
2025-06-13 14:30:00,511 DeMo.train INFO: Epoch[6] Iteration[30/65] Loss: 1.246, Acc: 0.972, Base Lr: 3.17e-04
2025-06-13 14:30:07,650 DeMo.train INFO: Epoch[6] Iteration[40/65] Loss: 1.220, Acc: 0.979, Base Lr: 3.17e-04
2025-06-13 14:30:14,841 DeMo.train INFO: Epoch[6] Iteration[50/65] Loss: 1.193, Acc: 0.983, Base Lr: 3.17e-04
2025-06-13 14:30:22,223 DeMo.train INFO: Epoch[6] Iteration[60/65] Loss: 1.172, Acc: 0.986, Base Lr: 3.17e-04
2025-06-13 14:30:24,532 DeMo.train INFO: Epoch 6 done. Time per batch: 0.762[s] Speed: 167.9[samples/s]
2025-06-13 14:30:24,545 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:30:24,545 DeMo.train INFO: Current is the ori feature testing!
2025-06-13 14:30:24,545 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:32:23,229 DeMo.train INFO: Validation Results - Epoch: 6
2025-06-13 14:32:23,231 DeMo.train INFO: mAP: 81.7%
2025-06-13 14:32:23,231 DeMo.train INFO: CMC curve, Rank-1  :95.4%
2025-06-13 14:32:23,231 DeMo.train INFO: CMC curve, Rank-5  :95.9%
2025-06-13 14:32:23,231 DeMo.train INFO: CMC curve, Rank-10 :96.0%
2025-06-13 14:32:23,231 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:32:23,355 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:32:23,355 DeMo.train INFO: Current is the moe feature testing!
2025-06-13 14:32:23,355 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:34:23,322 DeMo.train INFO: Validation Results - Epoch: 6
2025-06-13 14:34:23,322 DeMo.train INFO: mAP: 73.8%
2025-06-13 14:34:23,322 DeMo.train INFO: CMC curve, Rank-1  :89.4%
2025-06-13 14:34:23,322 DeMo.train INFO: CMC curve, Rank-5  :90.2%
2025-06-13 14:34:23,322 DeMo.train INFO: CMC curve, Rank-10 :90.7%
2025-06-13 14:34:23,322 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:34:23,618 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:34:23,618 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-13 14:34:23,618 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:36:23,714 DeMo.train INFO: Validation Results - Epoch: 6
2025-06-13 14:36:23,714 DeMo.train INFO: mAP: 81.7%
2025-06-13 14:36:23,714 DeMo.train INFO: CMC curve, Rank-1  :95.5%
2025-06-13 14:36:23,714 DeMo.train INFO: CMC curve, Rank-5  :95.8%
2025-06-13 14:36:23,714 DeMo.train INFO: CMC curve, Rank-10 :95.9%
2025-06-13 14:36:23,714 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:36:24,823 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:36:24,823 DeMo.train INFO: Best mAP: 83.5%
2025-06-13 14:36:24,823 DeMo.train INFO: Best Rank-1: 95.5%
2025-06-13 14:36:24,823 DeMo.train INFO: Best Rank-5: 95.5%
2025-06-13 14:36:24,823 DeMo.train INFO: Best Rank-10: 95.9%
2025-06-13 14:36:24,823 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:36:34,531 DeMo.train INFO: Epoch[7] Iteration[10/65] Loss: 1.010, Acc: 0.990, Base Lr: 3.05e-04
2025-06-13 14:36:41,691 DeMo.train INFO: Epoch[7] Iteration[20/65] Loss: 1.012, Acc: 0.994, Base Lr: 3.05e-04
2025-06-13 14:36:48,864 DeMo.train INFO: Epoch[7] Iteration[30/65] Loss: 1.009, Acc: 0.996, Base Lr: 3.05e-04
2025-06-13 14:36:56,074 DeMo.train INFO: Epoch[7] Iteration[40/65] Loss: 0.993, Acc: 0.997, Base Lr: 3.05e-04
2025-06-13 14:37:03,276 DeMo.train INFO: Epoch[7] Iteration[50/65] Loss: 0.982, Acc: 0.998, Base Lr: 3.05e-04
2025-06-13 14:37:10,469 DeMo.train INFO: Epoch[7] Iteration[60/65] Loss: 0.972, Acc: 0.998, Base Lr: 3.05e-04
2025-06-13 14:37:12,798 DeMo.train INFO: Epoch 7 done. Time per batch: 0.762[s] Speed: 168.1[samples/s]
2025-06-13 14:37:12,809 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:37:12,809 DeMo.train INFO: Current is the ori feature testing!
2025-06-13 14:37:12,809 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:39:11,559 DeMo.train INFO: Validation Results - Epoch: 7
2025-06-13 14:39:11,560 DeMo.train INFO: mAP: 84.2%
2025-06-13 14:39:11,560 DeMo.train INFO: CMC curve, Rank-1  :95.9%
2025-06-13 14:39:11,560 DeMo.train INFO: CMC curve, Rank-5  :96.2%
2025-06-13 14:39:11,560 DeMo.train INFO: CMC curve, Rank-10 :96.3%
2025-06-13 14:39:11,560 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:39:11,686 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:39:11,687 DeMo.train INFO: Current is the moe feature testing!
2025-06-13 14:39:11,687 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:41:10,216 DeMo.train INFO: Validation Results - Epoch: 7
2025-06-13 14:41:10,216 DeMo.train INFO: mAP: 75.8%
2025-06-13 14:41:10,216 DeMo.train INFO: CMC curve, Rank-1  :88.4%
2025-06-13 14:41:10,217 DeMo.train INFO: CMC curve, Rank-5  :88.9%
2025-06-13 14:41:10,217 DeMo.train INFO: CMC curve, Rank-10 :89.7%
2025-06-13 14:41:10,217 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:41:10,232 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-13 14:41:10,232 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-13 14:41:10,233 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
