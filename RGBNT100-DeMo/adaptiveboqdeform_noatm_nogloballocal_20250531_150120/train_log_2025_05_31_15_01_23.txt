2025-05-31 15:01:23,442 DeMo INFO: Saving model in the path :./RGBNT100-DeMo/adaptiveboqdeform_noatm_nogloballocal_20250531_150120
2025-05-31 15:01:23,443 DeMo INFO: Namespace(config_file='configs/RGBNT100/DeMo.yml', fea_cft=0, local_rank=0, opts=['OUTPUT_DIR', './RGBNT100-DeMo/adaptiveboqdeform_noatm_nogloballocal_20250531_150120'])
2025-05-31 15:01:23,443 DeMo INFO: Loaded configuration file configs/RGBNT100/DeMo.yml
2025-05-31 15:01:23,443 DeMo INFO: 
MODEL:
  TRANSFORMER_TYPE: 'ViT-B-16'
  STRIDE_SIZE: [ 16, 16 ]
  SIE_CAMERA: True
  DIRECT: 0
  SIE_COE: 1.0
  ID_LOSS_WEIGHT: 0.25
  TRIPLET_LOSS_WEIGHT: 1.0
  GLOBAL_LOCAL: True
  HDM: True
  ATM: False
  HEAD: 8
  FROZEN: False
  CENGJIFUSION: False

INPUT:
  SIZE_TRAIN: [ 128, 256 ]
  SIZE_TEST: [ 128, 256 ]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.5 # random erasing
  PADDING: 10

DATALOADER:
  SAMPLER: 'softmax_triplet'
  NUM_INSTANCE: 16
  NUM_WORKERS: 14

DATASETS:
  NAMES: ('RGBNT100')
  ROOT_DIR: '..'

SOLVER:
  BASE_LR: 0.00035
  WARMUP_ITERS: 5
  MAX_EPOCHS: 30
  OPTIMIZER_NAME: 'Adam'
  GAMMA: 0.1
  IMS_PER_BATCH: 128
  EVAL_PERIOD: 1
  SEED: 1111

TEST:
  IMS_PER_BATCH: 128
  RE_RANKING: 'no'
  WEIGHT: ''
  NECK_FEAT: 'before'
  FEAT_NORM: 'yes'
  MISS: "nothing"

OUTPUT_DIR: '..'




2025-05-31 15:01:23,443 DeMo INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 16
  NUM_WORKERS: 14
  SAMPLER: softmax_triplet
DATASETS:
  NAMES: RGBNT100
  ROOT_DIR: ..
INPUT:
  PADDING: 10
  PIXEL_MEAN: [0.5, 0.5, 0.5]
  PIXEL_STD: [0.5, 0.5, 0.5]
  PROB: 0.5
  RE_PROB: 0.5
  SIZE_TEST: [128, 256]
  SIZE_TRAIN: [128, 256]
MODEL:
  ADAPTER: False
  ATM: False
  ATT_DROP_RATE: 0.0
  CENGJIFUSION: False
  DEVICE: cuda
  DEVICE_ID: 0
  DIRECT: 0
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FROZEN: False
  GLOBAL_LOCAL: True
  HDM: True
  HEAD: 8
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 0.25
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  METRIC_LOSS_TYPE: triplet
  NAME: DeMo
  NECK: bnneck
  NO_MARGIN: True
  PRETRAIN_PATH_T: /path/to/your/vitb_16_224_21k.pth
  PROMPT: False
  SIE_CAMERA: True
  SIE_COE: 1.0
  SIE_VIEW: False
  STRIDE_SIZE: [16, 16]
  TRANSFORMER_TYPE: ViT-B-16
  TRIPLET_LOSS_WEIGHT: 1.0
OUTPUT_DIR: ./RGBNT100-DeMo/adaptiveboqdeform_noatm_nogloballocal_20250531_150120
SOLVER:
  BASE_LR: 0.00035
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 60
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.1
  IMS_PER_BATCH: 128
  LARGE_FC_LR: False
  LOG_PERIOD: 10
  MARGIN: 0.3
  MAX_EPOCHS: 30
  MOMENTUM: 0.9
  OPTIMIZER_NAME: Adam
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1111
  STEPS: (40, 70)
  WARMUP_FACTOR: 0.01
  WARMUP_ITERS: 5
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
TEST:
  FEAT: 0
  FEAT_NORM: yes
  IMS_PER_BATCH: 128
  MISS: nothing
  NECK_FEAT: before
  RE_RANKING: no
  WEIGHT: 
2025-05-31 15:01:30,521 DeMo INFO: combineway: adaptiveboqdeform
2025-05-31 15:01:31,023 DeMo INFO: DeMo(
  (BACKBONE): build_transformer(
    (base): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (pool): AdaptiveAvgPool1d(output_size=1)
  (rgb_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (nir_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (tir_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (generalFusion): GeneralFusion(
    (modalityboq): ModalitySpecificBoQ(
      (single_modal_boq): AdaptiveBoQ(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncoding()
        (boqs): ModuleList(
          (0): ImprovedBoQBlock(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlock(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=64, out_features=57, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlock(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=57, out_features=51, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlock(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=51, out_features=44, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusion(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=216, out_features=1, bias=True)
        )
      )
      (dual_modal_boq): AdaptiveBoQ(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncoding()
        (boqs): ModuleList(
          (0): ImprovedBoQBlock(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlock(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=76, out_features=68, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlock(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=68, out_features=60, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlock(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=60, out_features=53, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusion(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=257, out_features=1, bias=True)
        )
      )
      (triple_modal_boq): AdaptiveBoQ(
        (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (pos_encoding): PositionalEncoding()
        (boqs): ModuleList(
          (0): ImprovedBoQBlock(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ImprovedBoQBlock(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=96, out_features=86, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ImprovedBoQBlock(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=86, out_features=76, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ImprovedBoQBlock(
            (encoder): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (linear1): Linear(in_features=512, out_features=2048, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=2048, out_features=512, bias=True)
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
            )
            (query_projection): Linear(in_features=512, out_features=512, bias=True)
            (query_adapter): Linear(in_features=76, out_features=67, bias=True)
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (adaptive_fusion): AdaptiveFusion(
          (attention_net): Sequential(
            (0): Linear(in_features=512, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=1, bias=True)
          )
          (final_proj): Linear(in_features=325, out_features=1, bias=True)
        )
      )
    )
    (deformselect): DAttentionBaseline(
      (conv_offset): Sequential(
        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), groups=512)
        (3): GELU(approximate='none')
        (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (proj_q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attn_drop): Dropout(p=0.0, inplace=False)
    )
  )
  (classifier_moe): Linear(in_features=3584, out_features=50, bias=False)
  (bottleneck_moe): BatchNorm1d(3584, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier_r): Linear(in_features=512, out_features=50, bias=False)
  (bottleneck_r): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier_n): Linear(in_features=512, out_features=50, bias=False)
  (bottleneck_n): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier_t): Linear(in_features=512, out_features=50, bias=False)
  (bottleneck_t): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
2025-05-31 15:01:31,024 DeMo INFO: number of parameters:156.707068
2025-05-31 15:01:31,091 DeMo.train INFO: start training
2025-05-31 15:01:50,096 DeMo.train INFO: Epoch[1] Iteration[10/65] Loss: 9.756, Acc: 0.071, Base Lr: 9.80e-05
2025-05-31 15:01:56,479 DeMo.train INFO: Epoch[1] Iteration[20/65] Loss: 8.414, Acc: 0.143, Base Lr: 9.80e-05
2025-05-31 15:02:02,953 DeMo.train INFO: Epoch[1] Iteration[30/65] Loss: 7.904, Acc: 0.207, Base Lr: 9.80e-05
2025-05-31 15:02:09,368 DeMo.train INFO: Epoch[1] Iteration[40/65] Loss: 7.630, Acc: 0.269, Base Lr: 9.80e-05
2025-05-31 15:02:15,770 DeMo.train INFO: Epoch[1] Iteration[50/65] Loss: 7.449, Acc: 0.319, Base Lr: 9.80e-05
2025-05-31 15:02:22,190 DeMo.train INFO: Epoch[1] Iteration[60/65] Loss: 7.314, Acc: 0.361, Base Lr: 9.80e-05
2025-05-31 15:02:24,333 DeMo.train INFO: Epoch 1 done. Time per batch: 0.835[s] Speed: 153.4[samples/s]
2025-05-31 15:02:24,336 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:02:24,336 DeMo.train INFO: Current is the ori feature testing!
2025-05-31 15:02:24,336 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:04:21,671 DeMo.train INFO: Validation Results - Epoch: 1
2025-05-31 15:04:21,672 DeMo.train INFO: mAP: 55.7%
2025-05-31 15:04:21,672 DeMo.train INFO: CMC curve, Rank-1  :80.2%
2025-05-31 15:04:21,672 DeMo.train INFO: CMC curve, Rank-5  :81.5%
2025-05-31 15:04:21,672 DeMo.train INFO: CMC curve, Rank-10 :82.9%
2025-05-31 15:04:21,672 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:04:21,920 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:04:21,920 DeMo.train INFO: Current is the moe feature testing!
2025-05-31 15:04:21,920 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:06:18,701 DeMo.train INFO: Validation Results - Epoch: 1
2025-05-31 15:06:18,702 DeMo.train INFO: mAP: 54.8%
2025-05-31 15:06:18,702 DeMo.train INFO: CMC curve, Rank-1  :77.0%
2025-05-31 15:06:18,702 DeMo.train INFO: CMC curve, Rank-5  :78.0%
2025-05-31 15:06:18,702 DeMo.train INFO: CMC curve, Rank-10 :79.3%
2025-05-31 15:06:18,702 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:06:19,498 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:06:19,498 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-05-31 15:06:19,499 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:08:15,719 DeMo.train INFO: Validation Results - Epoch: 1
2025-05-31 15:08:15,719 DeMo.train INFO: mAP: 57.3%
2025-05-31 15:08:15,719 DeMo.train INFO: CMC curve, Rank-1  :82.1%
2025-05-31 15:08:15,719 DeMo.train INFO: CMC curve, Rank-5  :83.9%
2025-05-31 15:08:15,719 DeMo.train INFO: CMC curve, Rank-10 :84.9%
2025-05-31 15:08:15,720 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:08:21,689 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:08:21,689 DeMo.train INFO: Best mAP: 57.3%
2025-05-31 15:08:21,690 DeMo.train INFO: Best Rank-1: 82.1%
2025-05-31 15:08:21,690 DeMo.train INFO: Best Rank-5: 83.9%
2025-05-31 15:08:21,690 DeMo.train INFO: Best Rank-10: 84.9%
2025-05-31 15:08:21,690 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:08:31,493 DeMo.train INFO: Epoch[2] Iteration[10/65] Loss: 6.382, Acc: 0.504, Base Lr: 1.61e-04
2025-05-31 15:08:38,454 DeMo.train INFO: Epoch[2] Iteration[20/65] Loss: 6.204, Acc: 0.557, Base Lr: 1.61e-04
2025-05-31 15:08:44,844 DeMo.train INFO: Epoch[2] Iteration[30/65] Loss: 6.062, Acc: 0.581, Base Lr: 1.61e-04
2025-05-31 15:08:51,238 DeMo.train INFO: Epoch[2] Iteration[40/65] Loss: 5.890, Acc: 0.620, Base Lr: 1.61e-04
2025-05-31 15:08:57,620 DeMo.train INFO: Epoch[2] Iteration[50/65] Loss: 5.725, Acc: 0.672, Base Lr: 1.61e-04
2025-05-31 15:09:04,070 DeMo.train INFO: Epoch[2] Iteration[60/65] Loss: 5.562, Acc: 0.704, Base Lr: 1.61e-04
2025-05-31 15:09:05,593 DeMo.train INFO: Epoch 2 done. Time per batch: 0.708[s] Speed: 180.8[samples/s]
2025-05-31 15:09:05,612 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:09:05,613 DeMo.train INFO: Current is the ori feature testing!
2025-05-31 15:09:05,613 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:11:01,931 DeMo.train INFO: Validation Results - Epoch: 2
2025-05-31 15:11:01,931 DeMo.train INFO: mAP: 66.3%
2025-05-31 15:11:01,931 DeMo.train INFO: CMC curve, Rank-1  :82.4%
2025-05-31 15:11:01,932 DeMo.train INFO: CMC curve, Rank-5  :83.5%
2025-05-31 15:11:01,932 DeMo.train INFO: CMC curve, Rank-10 :83.8%
2025-05-31 15:11:01,932 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:11:02,115 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:11:02,116 DeMo.train INFO: Current is the moe feature testing!
2025-05-31 15:11:02,116 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:12:59,132 DeMo.train INFO: Validation Results - Epoch: 2
2025-05-31 15:12:59,132 DeMo.train INFO: mAP: 69.6%
2025-05-31 15:12:59,132 DeMo.train INFO: CMC curve, Rank-1  :85.8%
2025-05-31 15:12:59,133 DeMo.train INFO: CMC curve, Rank-5  :87.0%
2025-05-31 15:12:59,133 DeMo.train INFO: CMC curve, Rank-10 :87.7%
2025-05-31 15:12:59,133 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:12:59,425 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:12:59,426 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-05-31 15:12:59,426 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:14:56,113 DeMo.train INFO: Validation Results - Epoch: 2
2025-05-31 15:14:56,114 DeMo.train INFO: mAP: 69.2%
2025-05-31 15:14:56,114 DeMo.train INFO: CMC curve, Rank-1  :85.2%
2025-05-31 15:14:56,114 DeMo.train INFO: CMC curve, Rank-5  :86.4%
2025-05-31 15:14:56,114 DeMo.train INFO: CMC curve, Rank-10 :87.0%
2025-05-31 15:14:56,114 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:15:02,803 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:15:02,804 DeMo.train INFO: Best mAP: 69.2%
2025-05-31 15:15:02,804 DeMo.train INFO: Best Rank-1: 85.2%
2025-05-31 15:15:02,804 DeMo.train INFO: Best Rank-5: 86.4%
2025-05-31 15:15:02,804 DeMo.train INFO: Best Rank-10: 87.0%
2025-05-31 15:15:02,804 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:15:12,284 DeMo.train INFO: Epoch[3] Iteration[10/65] Loss: 4.488, Acc: 0.813, Base Lr: 2.24e-04
2025-05-31 15:15:18,693 DeMo.train INFO: Epoch[3] Iteration[20/65] Loss: 4.414, Acc: 0.844, Base Lr: 2.24e-04
2025-05-31 15:15:25,138 DeMo.train INFO: Epoch[3] Iteration[30/65] Loss: 4.248, Acc: 0.870, Base Lr: 2.24e-04
2025-05-31 15:15:31,586 DeMo.train INFO: Epoch[3] Iteration[40/65] Loss: 4.079, Acc: 0.887, Base Lr: 2.24e-04
2025-05-31 15:15:37,992 DeMo.train INFO: Epoch[3] Iteration[50/65] Loss: 3.923, Acc: 0.900, Base Lr: 2.24e-04
2025-05-31 15:15:44,384 DeMo.train INFO: Epoch[3] Iteration[60/65] Loss: 3.845, Acc: 0.910, Base Lr: 2.24e-04
2025-05-31 15:15:46,473 DeMo.train INFO: Epoch 3 done. Time per batch: 0.693[s] Speed: 184.7[samples/s]
2025-05-31 15:15:46,486 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:15:46,486 DeMo.train INFO: Current is the ori feature testing!
2025-05-31 15:15:46,486 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:17:42,100 DeMo.train INFO: Validation Results - Epoch: 3
2025-05-31 15:17:42,100 DeMo.train INFO: mAP: 74.4%
2025-05-31 15:17:42,100 DeMo.train INFO: CMC curve, Rank-1  :90.2%
2025-05-31 15:17:42,100 DeMo.train INFO: CMC curve, Rank-5  :90.7%
2025-05-31 15:17:42,100 DeMo.train INFO: CMC curve, Rank-10 :90.9%
2025-05-31 15:17:42,100 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:17:42,234 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-05-31 15:17:42,234 DeMo.train INFO: Current is the moe feature testing!
2025-05-31 15:17:42,235 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
