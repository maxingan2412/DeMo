import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import einops
from typing import Dict, List, Optional, Tuple


class CMQEConfig:
    """
    CMQEæ¶ˆèå®éªŒé…ç½®ç±» - æ§åˆ¶å„ç»„ä»¶çš„å¼€å¯/å…³é—­
    """

    def __init__(self,
                 enable_hierarchical_queries=True,  # HQ: Progressive query learning (å±‚æ¬¡åŒ–æŸ¥è¯¢å­¦ä¹ )
                 enable_query_propagation=True,  # QP: Inter-layer query propagation (è·¨å±‚æŸ¥è¯¢ä¼ æ’­)
                 enable_cross_modal_exchange=True,  # CMQE: Cross-Modal Query Exchange (è·¨æ¨¡æ€æŸ¥è¯¢äº¤æ¢)
                 enable_adaptive_fusion=True,  # AF: Adaptive feature fusion (è‡ªé€‚åº”ç‰¹å¾èåˆ)
                 exchange_ratio=0.2):  # CMQEäº¤æ¢å¼ºåº¦
        self.enable_hierarchical_queries = enable_hierarchical_queries
        self.enable_query_propagation = enable_query_propagation
        self.enable_cross_modal_exchange = enable_cross_modal_exchange
        self.enable_adaptive_fusion = enable_adaptive_fusion
        self.exchange_ratio = exchange_ratio

    def get_ablation_name(self):
        """ç”Ÿæˆæ¶ˆèå®éªŒçš„åç§°æ ‡è¯†"""
        components = []
        if self.enable_hierarchical_queries: components.append("HQ")
        if self.enable_query_propagation: components.append("QP")
        if self.enable_cross_modal_exchange: components.append("CMQE")
        if self.enable_adaptive_fusion: components.append("AF")
        return "_".join(components) if components else "CMQEBaseline"


class CMQEPositionalEncoding(nn.Module):
    """CMQEç³»ç»Ÿçš„ä½ç½®ç¼–ç """

    def __init__(self, d_model, max_len=5000):
        super().__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe)
        self.pos_weight = nn.Parameter(torch.ones(1))

    def forward(self, x):
        seq_len = x.size(1)
        pos_enc = self.pe[:seq_len].unsqueeze(0)
        return x + self.pos_weight * pos_enc


class CMQEBoQBlock(torch.nn.Module):
    """CMQE BoQ Block with Query State Access"""

    def __init__(self, in_dim, num_queries, nheads=8, layer_idx=0, total_layers=2,
                 prev_num_queries=None, cmqe_config=None):
        super(CMQEBoQBlock, self).__init__()

        self.layer_idx = layer_idx
        self.total_layers = total_layers
        self.num_queries = num_queries
        self.in_dim = in_dim
        self.config = cmqe_config if cmqe_config is not None else CMQEConfig()

        self.encoder = torch.nn.TransformerEncoderLayer(
            d_model=in_dim, nhead=nheads, dim_feedforward=4 * in_dim,
            batch_first=True, dropout=0.)

        # ç»„ä»¶1: Progressive Query Learning (å±‚æ¬¡åŒ–æŸ¥è¯¢å­¦ä¹ )
        if self.config.enable_hierarchical_queries:
            init_scale = 1 * (1 + layer_idx * 0.5)
            print(f"[CMQE Ablation] âœ“ Layer {layer_idx}: Hierarchical queries enabled with scale {init_scale:.3f}")
        else:
            init_scale = 1
            print(
                f"[CMQE Ablation] âœ— Layer {layer_idx}: Hierarchical queries disabled, using fixed scale {init_scale:.3f}")

        self.queries = torch.nn.Parameter(torch.randn(1, num_queries, in_dim) * init_scale)

        # æŸ¥è¯¢ä¸“é—¨åŒ–æŠ•å½±
        self.query_projection = nn.Linear(in_dim, in_dim)

        # ç»„ä»¶2: Inter-layer Query Propagation (è·¨å±‚æŸ¥è¯¢ä¼ æ’­)
        if self.config.enable_query_propagation and prev_num_queries is not None and prev_num_queries != num_queries:
            self.query_adapter = nn.Linear(prev_num_queries, num_queries)
            print(
                f"[CMQE Ablation] âœ“ Layer {layer_idx}: Query propagation enabled with adapter ({prev_num_queries}->{num_queries})")
        elif self.config.enable_query_propagation:
            self.query_adapter = None
            print(f"[CMQE Ablation] âœ“ Layer {layer_idx}: Query propagation enabled (no adapter needed)")
        else:
            self.query_adapter = None
            print(f"[CMQE Ablation] âœ— Layer {layer_idx}: Query propagation disabled")

        self.self_attn = torch.nn.MultiheadAttention(in_dim, num_heads=nheads, batch_first=True)
        self.norm_q = torch.nn.LayerNorm(in_dim)

        self.cross_attn = torch.nn.MultiheadAttention(in_dim, num_heads=nheads, batch_first=True)
        self.norm_out = torch.nn.LayerNorm(in_dim)

        # æ®‹å·®é—¨æ§æœºåˆ¶ (ä»…åœ¨æŸ¥è¯¢ä¼ æ’­å¯ç”¨æ—¶ä½¿ç”¨)
        if self.config.enable_query_propagation:
            self.gate = nn.Parameter(torch.ones(1))

    def forward(self, x, prev_queries=None, external_queries=None):
        """
        Args:
            x: input features [B, N, D]
            prev_queries: queries from previous layer [B, num_queries, D]
            external_queries: queries from CMQE [B, num_queries, D]
        """
        B = x.size(0)
        x = self.encoder(x)

        # åˆå§‹åŒ–æŸ¥è¯¢
        if external_queries is not None:
            # ä½¿ç”¨å¤–éƒ¨å¢å¼ºæŸ¥è¯¢ (æ¥è‡ªCMQE)
            q = external_queries
        else:
            q = self.queries.repeat(B, 1, 1)

        # ç»„ä»¶2: å±‚é—´æŸ¥è¯¢ä¼ é€’
        if self.config.enable_query_propagation and prev_queries is not None and self.layer_idx > 0:
            if self.query_adapter is not None:
                adapted_queries = self.query_adapter(prev_queries.permute(0, 2, 1)).permute(0, 2, 1)
            else:
                adapted_queries = prev_queries

            alpha = torch.sigmoid(self.gate)
            q = alpha * q + (1 - alpha) * adapted_queries

        # æŸ¥è¯¢ä¸“é—¨åŒ–æŠ•å½±
        q = self.query_projection(q)

        # è‡ªæ³¨æ„åŠ›
        q_self_attn = q + self.self_attn(q, q, q)[0]
        q_normed = self.norm_q(q_self_attn)

        # äº¤å‰æ³¨æ„åŠ›
        out, attn = self.cross_attn(q_normed, x, x)
        out = self.norm_out(out)

        return x, out, attn.detach(), q_normed


class CrossModalQueryExchange(nn.Module):
    """
    è·¨æ¨¡æ€æŸ¥è¯¢äº¤æ¢æœºåˆ¶ (CMQE)

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å­¦ä¹ æ¨¡æ€é—´ç›¸ä¼¼æ€§çŸ©é˜µ
    2. æ™ºèƒ½è·¨æ¨¡æ€æŸ¥è¯¢ä¿¡æ¯äº¤æ¢
    3. åŠ¨æ€è°ƒèŠ‚äº¤æ¢å¼ºåº¦
    """

    def __init__(self, query_dim, num_heads=8, exchange_ratio=0.2):
        super().__init__()

        self.query_dim = query_dim
        self.num_heads = num_heads
        self.exchange_ratio = exchange_ratio

        # è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶
        self.cross_modal_attn = nn.MultiheadAttention(
            query_dim, num_heads, batch_first=True, dropout=0.1
        )

        # æŸ¥è¯¢èåˆç½‘ç»œ
        self.query_fusion = nn.Sequential(
            nn.Linear(query_dim * 2, query_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(query_dim, query_dim)
        )

        # äº¤æ¢å¼ºåº¦æ§åˆ¶ (å¯å­¦ä¹ å‚æ•°)
        self.exchange_gate = nn.Parameter(torch.tensor(exchange_ratio))

        # æ¨¡æ€ç›¸ä¼¼æ€§çŸ©é˜µ (å¯å­¦ä¹ ) 7x7çŸ©é˜µ
        self.modality_similarity = nn.Parameter(torch.eye(7) + 0.1 * torch.randn(7, 7))

        # æŸ¥è¯¢å¤§å°é€‚é…å™¨
        self.query_adapters = nn.ModuleDict()

    def _get_modality_index(self, modality):
        """è·å–æ¨¡æ€åœ¨ç›¸ä¼¼æ€§çŸ©é˜µä¸­çš„ç´¢å¼•"""
        mapping = {
            'RGB': 0, 'NI': 1, 'TI': 2,
            'RGB_NI': 3, 'RGB_TI': 4, 'NI_TI': 5, 'RGB_NI_TI': 6
        }
        return mapping.get(modality, 0)

    def _adapt_query_size(self, source_queries, target_num_queries):
        """é€‚é…ä¸åŒæ¨¡æ€é—´çš„æŸ¥è¯¢æ•°é‡å·®å¼‚"""
        B, N_source, D = source_queries.shape

        if N_source == target_num_queries:
            return source_queries

        # åˆ›å»ºæˆ–è·å–é€‚é…å™¨
        adapter_key = f"{N_source}_to_{target_num_queries}"
        if adapter_key not in self.query_adapters:
            self.query_adapters[adapter_key] = nn.Linear(N_source, target_num_queries).to(source_queries.device)

        # ä½¿ç”¨çº¿æ€§é€‚é…å™¨è°ƒæ•´æŸ¥è¯¢æ•°é‡
        adapted = self.query_adapters[adapter_key](source_queries.permute(0, 2, 1)).permute(0, 2, 1)
        return adapted

    def forward(self, query_dict):
        """
        Args:
            query_dict: {modality: queries} where queries is [B, N, D]
        Returns:
            enhanced_query_dict: å¢å¼ºåçš„æŸ¥è¯¢å­—å…¸
        """
        enhanced_queries = {}

        # è®¡ç®—æ¨¡æ€ç›¸ä¼¼æ€§æƒé‡ (softmaxå½’ä¸€åŒ–)
        similarity_matrix = torch.softmax(self.modality_similarity, dim=1)

        for modal, queries in query_dict.items():
            modal_idx = self._get_modality_index(modal)
            B, N, D = queries.shape

            # è·å–ä¸å…¶ä»–æ¨¡æ€çš„ç›¸ä¼¼æ€§æƒé‡
            similarity_weights = similarity_matrix[modal_idx]

            # æ”¶é›†æ¥è‡ªå…¶ä»–æ¨¡æ€çš„æŸ¥è¯¢ä¿¡æ¯
            cross_modal_features = []
            valid_weights = []

            for other_modal, other_queries in query_dict.items():
                if other_modal != modal:
                    other_idx = self._get_modality_index(other_modal)
                    weight = similarity_weights[other_idx]

                    # åªè€ƒè™‘ç›¸ä¼¼æ€§æƒé‡å¤§äºé˜ˆå€¼çš„æ¨¡æ€
                    if weight > 0.1:
                        # é€‚é…æŸ¥è¯¢å¤§å°
                        adapted_queries = self._adapt_query_size(other_queries, N)
                        cross_modal_features.append(adapted_queries)
                        valid_weights.append(weight)

            if cross_modal_features and len(valid_weights) > 0:
                # åŠ æƒèåˆå…¶ä»–æ¨¡æ€çš„æŸ¥è¯¢
                valid_weights = torch.tensor(valid_weights, device=queries.device)
                valid_weights = valid_weights / valid_weights.sum()  # å½’ä¸€åŒ–

                weighted_cross_queries = torch.zeros_like(queries)
                for cross_queries, weight in zip(cross_modal_features, valid_weights):
                    weighted_cross_queries += weight * cross_queries

                # é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›å¢å¼ºåŸå§‹æŸ¥è¯¢
                enhanced_q, attn_weights = self.cross_modal_attn(
                    queries, weighted_cross_queries, weighted_cross_queries
                )

                # æŸ¥è¯¢èåˆ
                concatenated = torch.cat([queries, enhanced_q], dim=-1)
                fused_queries = self.query_fusion(concatenated)

                # é—¨æ§æœºåˆ¶ï¼šæ§åˆ¶è·¨æ¨¡æ€ä¿¡æ¯çš„èåˆæ¯”ä¾‹
                gate_weight = torch.sigmoid(self.exchange_gate)
                final_queries = gate_weight * fused_queries + (1 - gate_weight) * queries

                enhanced_queries[modal] = final_queries
            else:
                # æ²¡æœ‰æœ‰æ•ˆçš„è·¨æ¨¡æ€ä¿¡æ¯ï¼Œä¿æŒåŸå§‹æŸ¥è¯¢
                enhanced_queries[modal] = queries

        return enhanced_queries


class CMQEAdaptiveBoQ(torch.nn.Module):
    """CMQEç³»ç»Ÿçš„è‡ªé€‚åº”BoQæ¨¡å—"""

    def __init__(self, input_dim=512, num_queries=32, num_layers=2, row_dim=32,
                 use_positional_encoding=True, cmqe_config=None):
        super().__init__()

        self.use_positional_encoding = use_positional_encoding
        self.norm_input = torch.nn.LayerNorm(input_dim)
        self.config = cmqe_config if cmqe_config is not None else CMQEConfig()
        self.input_dim = input_dim
        self.num_layers = num_layers

        if use_positional_encoding:
            self.pos_encoding = CMQEPositionalEncoding(input_dim)

        # ç»„ä»¶1: å±‚ç‰¹å¼‚æ€§çš„æŸ¥è¯¢æ•°é‡ (å±‚æ¬¡åŒ–æŸ¥è¯¢å­¦ä¹ )
        if self.config.enable_hierarchical_queries:
            layer_queries = self._get_layer_queries_hierarchical(num_queries, num_layers)
            print(f"[CMQE Ablation] âœ“ Hierarchical queries: {layer_queries}")
        else:
            layer_queries = [num_queries] * num_layers
            print(f"[CMQE Ablation] âœ— Hierarchical queries disabled: {layer_queries}")

        self.layer_queries = layer_queries
        self.boqs = torch.nn.ModuleList()

        for i in range(num_layers):
            prev_queries = layer_queries[i - 1] if i > 0 else None
            self.boqs.append(
                CMQEBoQBlock(input_dim, layer_queries[i],
                             nheads=max(1, input_dim // 64),
                             layer_idx=i, total_layers=num_layers,
                             prev_num_queries=prev_queries,
                             cmqe_config=self.config)
            )

        # ç»„ä»¶4: è‡ªé€‚åº”ç‰¹å¾èåˆ
        total_query_outputs = sum(layer_queries)
        self.adaptive_fusion = CMQEAdaptiveFusion(input_dim, total_query_outputs, row_dim, self.config)

    def _get_layer_queries_hierarchical(self, base_queries, num_layers):
        """å±‚æ¬¡åŒ–æŸ¥è¯¢åˆ†é… (ç»„ä»¶1)"""
        if num_layers == 1:
            return [base_queries]

        queries_per_layer = []
        for i in range(num_layers):
            ratio = 1.0 - (i / (num_layers - 1)) * 0.3
            layer_q = max(8, int(base_queries * ratio))
            queries_per_layer.append(layer_q)

        return queries_per_layer

    def forward(self, x, enhanced_queries=None):
        """
        Args:
            x: input features [B, N, D]
            enhanced_queries: æ¥è‡ªCMQEçš„å¢å¼ºæŸ¥è¯¢ (å¯é€‰)
        """
        if self.use_positional_encoding:
            x = self.pos_encoding(x)
        x = self.norm_input(x)

        outs = []
        attns = []
        prev_queries = None

        for i, boq_layer in enumerate(self.boqs):
            # å¦‚æœç¬¬ä¸€å±‚æœ‰å¢å¼ºæŸ¥è¯¢ï¼Œåˆ™ä½¿ç”¨ï¼›å¦åˆ™ä½¿ç”¨None
            external_q = enhanced_queries if i == 0 and enhanced_queries is not None else None

            x, out, attn, queries = boq_layer(x, prev_queries, external_q)
            outs.append(out)
            attns.append(attn)

            # ç»„ä»¶2: æŸ¥è¯¢ä¼ æ’­æ§åˆ¶
            if self.config.enable_query_propagation:
                prev_queries = queries
            else:
                prev_queries = None

        # ç»„ä»¶4: è‡ªé€‚åº”èåˆ
        final_out = self.adaptive_fusion(outs)

        # è¿”å›æœ€ç»ˆè¾“å‡ºã€æ³¨æ„åŠ›æƒé‡å’Œç¬¬ä¸€å±‚æŸ¥è¯¢(ç”¨äºCMQE)
        first_layer_queries = outs[0] if outs else None
        return final_out, attns, first_layer_queries


class CMQEAdaptiveFusion(nn.Module):
    """CMQEç³»ç»Ÿçš„è‡ªé€‚åº”ç‰¹å¾èåˆæ¨¡å—"""

    def __init__(self, feat_dim, total_queries, output_dim, cmqe_config=None):
        super().__init__()

        self.feat_dim = feat_dim
        self.total_queries = total_queries
        self.config = cmqe_config if cmqe_config is not None else CMQEConfig()

        if self.config.enable_adaptive_fusion:
            self.attention_net = nn.Sequential(
                nn.Linear(feat_dim, feat_dim // 4),
                nn.ReLU(),
                nn.Linear(feat_dim // 4, 1)
            )
            print("[CMQE Ablation] âœ“ Adaptive fusion enabled")
        else:
            print("[CMQE Ablation] âœ— Adaptive fusion disabled (using simple concatenation)")

        self.final_proj = nn.Linear(total_queries, output_dim)

    def forward(self, layer_outputs):
        if self.config.enable_adaptive_fusion:
            layer_weights = []
            for out in layer_outputs:
                global_feat = torch.mean(out, dim=1)
                weight = torch.sigmoid(self.attention_net(global_feat))
                layer_weights.append(weight)

            weighted_outputs = []
            for out, weight in zip(layer_outputs, layer_weights):
                weighted_out = out * weight.unsqueeze(1)
                weighted_outputs.append(weighted_out)
        else:
            weighted_outputs = layer_outputs

        concat_out = torch.cat(weighted_outputs, dim=1)
        final_out = self.final_proj(concat_out.permute(0, 2, 1))
        final_out = final_out.flatten(1)
        final_out = torch.nn.functional.normalize(final_out, p=2, dim=-1)

        return final_out


class CMQEModalitySystem(nn.Module):
    """
    CMQEå®Œæ•´çš„æ¨¡æ€ç‰¹å¼‚æ€§ç³»ç»Ÿ

    æ¶æ„ï¼š
    1. 7ä¸ªç‹¬ç«‹çš„BoQæ¨¡å— (åŸºç¡€è®¾è®¡)
    2. CMQEè·¨æ¨¡æ€æŸ¥è¯¢äº¤æ¢ (æ ¸å¿ƒåˆ›æ–°)
    3. ä¸¤é˜¶æ®µå¤„ç†ï¼šæŸ¥è¯¢äº¤æ¢ â†’ ç‰¹å¾æå–
    """

    def __init__(self, input_dim=512, num_queries=32, num_layers=2, row_dim=32, cmqe_config=None):
        super().__init__()

        self.config = cmqe_config if cmqe_config is not None else CMQEConfig()
        self.input_dim = input_dim

        print(f"[CMQE Ablation] Using configuration: {self.config.get_ablation_name()}")

        # 7ä¸ªç‹¬ç«‹çš„BoQæ¨¡å— (ç°åœ¨æ˜¯åŸºç¡€è®¾è®¡)
        self.boq_modules = nn.ModuleDict({
            'RGB': CMQEAdaptiveBoQ(input_dim, num_queries, num_layers, row_dim, True, self.config),
            'NI': CMQEAdaptiveBoQ(input_dim, num_queries, num_layers, row_dim, True, self.config),
            'TI': CMQEAdaptiveBoQ(input_dim, num_queries, num_layers, row_dim, True, self.config),
            'RGB_NI': CMQEAdaptiveBoQ(input_dim, int(num_queries * 1.2), num_layers, row_dim, True, self.config),
            'RGB_TI': CMQEAdaptiveBoQ(input_dim, int(num_queries * 1.2), num_layers, row_dim, True, self.config),
            'NI_TI': CMQEAdaptiveBoQ(input_dim, int(num_queries * 1.2), num_layers, row_dim, True, self.config),
            'RGB_NI_TI': CMQEAdaptiveBoQ(input_dim, int(num_queries * 1.5), num_layers, row_dim, True, self.config)
        })

        # ğŸš€ æ ¸å¿ƒåˆ›æ–°ï¼šè·¨æ¨¡æ€æŸ¥è¯¢äº¤æ¢æ¨¡å— (CMQE)
        if self.config.enable_cross_modal_exchange:
            self.cmqe_module = CrossModalQueryExchange(
                query_dim=input_dim,
                num_heads=8,
                exchange_ratio=self.config.exchange_ratio
            )
            print("[CMQE Ablation] âœ“ CMQE (Cross-Modal Query Exchange) enabled")
        else:
            self.cmqe_module = None
            print("[CMQE Ablation] âœ— CMQE (Cross-Modal Query Exchange) disabled")

    def forward(self, features_dict):
        """
        Args:
            features_dict: {modality: features} where features is [N, B, D]
        Returns:
            results: {modality: output}
            attentions: {modality: attention_weights}
        """
        # Phase 1: è·å–åˆå§‹æŸ¥è¯¢çŠ¶æ€
        initial_queries = {}

        for modality, feat in features_dict.items():
            feat = feat.permute(1, 0, 2)  # [N, B, D] -> [B, N, D]
            boq_module = self.boq_modules[modality]

            # ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œè·å–åˆå§‹æŸ¥è¯¢
            _, _, first_layer_queries = boq_module(feat)
            if first_layer_queries is not None:
                initial_queries[modality] = first_layer_queries
            else:
                # å¦‚æœæ²¡æœ‰æŸ¥è¯¢è¾“å‡ºï¼Œä½¿ç”¨é»˜è®¤å½¢çŠ¶
                B = feat.size(0)
                num_q = boq_module.layer_queries[0]
                initial_queries[modality] = torch.randn(B, num_q, self.input_dim, device=feat.device)

        # Phase 2: è·¨æ¨¡æ€æŸ¥è¯¢äº¤æ¢ (CMQEæ ¸å¿ƒåˆ›æ–°)
        if self.config.enable_cross_modal_exchange and self.cmqe_module is not None:
            enhanced_queries = self.cmqe_module(initial_queries)
            #print(f"[CMQE] Enhanced queries for {len(enhanced_queries)} modalities")
        else:
            enhanced_queries = initial_queries

        # Phase 3: ä½¿ç”¨å¢å¼ºæŸ¥è¯¢è¿›è¡Œæœ€ç»ˆç‰¹å¾æå–
        results = {}
        attentions = {}

        for modality, feat in features_dict.items():
            feat = feat.permute(1, 0, 2)  # [N, B, D] -> [B, N, D]
            boq_module = self.boq_modules[modality]

            # ä½¿ç”¨å¢å¼ºæŸ¥è¯¢è¿›è¡Œæœ€ç»ˆæ¨ç†
            enhanced_q = enhanced_queries.get(modality, None)
            final_out, attn_list, _ = boq_module(feat, enhanced_q)

            results[modality] = final_out
            attentions[modality] = attn_list

        return results, attentions

    def get_ablation_summary(self):
        """è·å–å½“å‰æ¶ˆèé…ç½®çš„æ€»ç»“"""
        enabled = []
        disabled = []

        components = [
            ("HQ", "Hierarchical Queries", self.config.enable_hierarchical_queries),
            ("QP", "Query Propagation", self.config.enable_query_propagation),
            ("CMQE", "Cross-Modal Query Exchange", self.config.enable_cross_modal_exchange),
            ("AF", "Adaptive Fusion", self.config.enable_adaptive_fusion)
        ]

        for abbr, full_name, enabled_flag in components:
            if enabled_flag:
                enabled.append(f"{abbr} ({full_name})")
            else:
                disabled.append(f"{abbr} ({full_name})")

        summary = f"""
=== CMQE Ablation Configuration ===
Configuration Name: {self.config.get_ablation_name()}
Total BoQ Modules: 7 (RGB, NI, TI, RGB_NI, RGB_TI, NI_TI, RGB_NI_TI)
Enabled Components: {', '.join(enabled) if enabled else 'None'}
Disabled Components: {', '.join(disabled) if disabled else 'None'}
CMQE Exchange Ratio: {self.config.exchange_ratio}
===================================
        """
        return summary.strip()


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç 
def create_cmqe_ablation_configs():
    """åˆ›å»ºä¸åŒçš„CMQEæ¶ˆèå®éªŒé…ç½®"""

    configs = {
        # åŸºçº¿é…ç½®ï¼šæ‰€æœ‰ç»„ä»¶ç¦ç”¨
        'cmqe_baseline': CMQEConfig(
            enable_hierarchical_queries=False,
            enable_query_propagation=False,
            enable_cross_modal_exchange=False,
            enable_adaptive_fusion=False
        ),

        # å•ä¸ªç»„ä»¶æµ‹è¯•
        'cmqe_only_hq': CMQEConfig(
            enable_hierarchical_queries=True,
            enable_query_propagation=False,
            enable_cross_modal_exchange=False,
            enable_adaptive_fusion=False
        ),

        'cmqe_only_exchange': CMQEConfig(
            enable_hierarchical_queries=False,
            enable_query_propagation=False,
            enable_cross_modal_exchange=True,
            enable_adaptive_fusion=False
        ),

        # å®Œæ•´é…ç½®ï¼šæ‰€æœ‰ç»„ä»¶å¯ç”¨
        'cmqe_full': CMQEConfig(
            enable_hierarchical_queries=True,
            enable_query_propagation=True,
            enable_cross_modal_exchange=True,
            enable_adaptive_fusion=True
        ),

        # CMQEå¼ºåº¦æµ‹è¯•
        'cmqe_weak_exchange': CMQEConfig(
            enable_cross_modal_exchange=True,
            exchange_ratio=0.1
        ),

        'cmqe_strong_exchange': CMQEConfig(
            enable_cross_modal_exchange=True,
            exchange_ratio=0.5
        )
    }

    return configs


def test_cmqe_model():
    """æµ‹è¯•CMQEæ¨¡å‹çš„åŠŸèƒ½"""

    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = CMQEConfig(enable_cross_modal_exchange=True)

    # åˆ›å»ºæ¨¡å‹
    model = CMQEModalitySystem(
        input_dim=512,
        num_queries=32,
        num_layers=2,
        row_dim=32,
        cmqe_config=config
    )

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    seq_len = 100
    feat_dim = 512

    test_features = {
        'RGB': torch.randn(seq_len, batch_size, feat_dim),
        'NI': torch.randn(seq_len, batch_size, feat_dim),
        'TI': torch.randn(seq_len, batch_size, feat_dim),
        'RGB_NI': torch.randn(seq_len * 2, batch_size, feat_dim),
        'RGB_TI': torch.randn(seq_len * 2, batch_size, feat_dim),
        'NI_TI': torch.randn(seq_len * 2, batch_size, feat_dim),
        'RGB_NI_TI': torch.randn(seq_len * 3, batch_size, feat_dim)
    }

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        results, attentions = model(test_features)

    # æ‰“å°ç»“æœ
    print("\n=== CMQE Test Results ===")
    for modality, result in results.items():
        print(f"{modality}: {result.shape}")

    print(f"\n{model.get_ablation_summary()}")

    return model, results, attentions


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    model, results, attentions = test_cmqe_model()

    # å±•ç¤ºä¸åŒé…ç½®
    print("\n=== Available CMQE Ablation Configurations ===")
    configs = create_cmqe_ablation_configs()
    for name, config in configs.items():
        print(f"{name}: {config.get_ablation_name()}")