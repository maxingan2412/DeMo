2025-06-05 15:08:43,357 DeMo INFO: Saving model in the path :./RGBNT201-DeMo/cmeboqablation_all_20250605_150840
2025-06-05 15:08:43,357 DeMo INFO: Namespace(config_file='configs/RGBNT201/DeMo.yml', fea_cft=0, local_rank=0, opts=['OUTPUT_DIR', './RGBNT201-DeMo/cmeboqablation_all_20250605_150840'])
2025-06-05 15:08:43,357 DeMo INFO: Loaded configuration file configs/RGBNT201/DeMo.yml
2025-06-05 15:08:43,357 DeMo INFO: 
MODEL:
  TRANSFORMER_TYPE: 'ViT-B-16'
  STRIDE_SIZE: [ 16, 16 ]
  SIE_CAMERA: True
  DIRECT: 1
  SIE_COE: 1.0
  ID_LOSS_WEIGHT: 0.25
  TRIPLET_LOSS_WEIGHT: 1.0
  GLOBAL_LOCAL: True
  HDM: True
  ATM: False
  HEAD: 4 # orginal 4
  FROZEN: False
  CENGJIFUSION: False
  NEWDEFORM: True

INPUT:
  SIZE_TRAIN: [ 256, 128 ]
  SIZE_TEST: [ 256, 128 ]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.5 # random erasing
  PADDING: 10

DATALOADER:
  SAMPLER: 'softmax_triplet'
  NUM_INSTANCE: 8
  NUM_WORKERS: 14

DATASETS:
  NAMES: ('RGBNT201')
  ROOT_DIR: '..'

SOLVER:
  BASE_LR: 0.00035
  WARMUP_ITERS: 10
  MAX_EPOCHS: 50
  OPTIMIZER_NAME: 'Adam'
  IMS_PER_BATCH: 64 # orginal 64
  EVAL_PERIOD: 1
  SEED: 1111

TEST:
  IMS_PER_BATCH: 128
  RE_RANKING: 'yes'
  WEIGHT: ''
  NECK_FEAT: 'before'
  FEAT_NORM: 'yes'
  MISS: "nothing"

OUTPUT_DIR: '..'



2025-06-05 15:08:43,358 DeMo INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 8
  NUM_WORKERS: 14
  SAMPLER: softmax_triplet
DATASETS:
  NAMES: RGBNT201
  ROOT_DIR: ..
INPUT:
  PADDING: 10
  PIXEL_MEAN: [0.5, 0.5, 0.5]
  PIXEL_STD: [0.5, 0.5, 0.5]
  PROB: 0.5
  RE_PROB: 0.5
  SIZE_TEST: [256, 128]
  SIZE_TRAIN: [256, 128]
MODEL:
  ADAPTER: False
  ATM: False
  ATT_DROP_RATE: 0.0
  CENGJIFUSION: False
  DEVICE: cuda
  DEVICE_ID: 0
  DIRECT: 1
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FROZEN: False
  GLOBAL_LOCAL: True
  HDM: True
  HEAD: 4
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 0.25
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  METRIC_LOSS_TYPE: triplet
  NAME: DeMo
  NECK: bnneck
  NEWDEFORM: True
  NO_MARGIN: True
  PRETRAIN_PATH_T: /path/to/your/vitb_16_224_21k.pth
  PROMPT: False
  SIE_CAMERA: True
  SIE_COE: 1.0
  SIE_VIEW: False
  STRIDE_SIZE: [16, 16]
  TRANSFORMER_TYPE: ViT-B-16
  TRIPLET_LOSS_WEIGHT: 1.0
OUTPUT_DIR: ./RGBNT201-DeMo/cmeboqablation_all_20250605_150840
SOLVER:
  BASE_LR: 0.00035
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 60
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.1
  IMS_PER_BATCH: 64
  LARGE_FC_LR: False
  LOG_PERIOD: 10
  MARGIN: 0.3
  MAX_EPOCHS: 50
  MOMENTUM: 0.9
  OPTIMIZER_NAME: Adam
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1111
  STEPS: (40, 70)
  WARMUP_FACTOR: 0.01
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
TEST:
  FEAT: 0
  FEAT_NORM: yes
  IMS_PER_BATCH: 128
  MISS: nothing
  NECK_FEAT: before
  RE_RANKING: yes
  WEIGHT: 
=> RGBNT201 loaded
Dataset statistics:
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   171 |     3951 |         4
  query    |    30 |      836 |         2
  gallery  |    30 |      836 |         2
  ----------------------------------------
data is ready
cengjifusion: False mxa
rwkvbackbone: False mxa
using Transformer_type: ViT-B-16 as a backbone
Resized position embedding: %s to %s torch.Size([197, 768]) torch.Size([129, 768])
Position embedding resize to height:16 width: 8
Successfully load ckpt!
<All keys matched successfully>
Loading pretrained model from CLIP
camera number is : 4
combineway: cmeboqablation mxa
2025-06-05 15:08:49,944 DeMo INFO: combineway: cmeboqablation
newdeform: True mxa
UsingDiversity: False mxa
[CMQE Ablation] Using configuration: HQ_QP_CMQE_AF
[CMQE Ablation] ✓ Hierarchical queries: [64, 57, 51, 44]
[CMQE Ablation] ✓ Layer 0: Hierarchical queries enabled with scale 1.000
[CMQE Ablation] ✓ Layer 0: Query propagation enabled (no adapter needed)
[CMQE Ablation] ✓ Layer 1: Hierarchical queries enabled with scale 1.500
[CMQE Ablation] ✓ Layer 1: Query propagation enabled with adapter (64->57)
[CMQE Ablation] ✓ Layer 2: Hierarchical queries enabled with scale 2.000
[CMQE Ablation] ✓ Layer 2: Query propagation enabled with adapter (57->51)
[CMQE Ablation] ✓ Layer 3: Hierarchical queries enabled with scale 2.500
[CMQE Ablation] ✓ Layer 3: Query propagation enabled with adapter (51->44)
[CMQE Ablation] ✓ Adaptive fusion enabled
[CMQE Ablation] ✓ Hierarchical queries: [64, 57, 51, 44]
[CMQE Ablation] ✓ Layer 0: Hierarchical queries enabled with scale 1.000
[CMQE Ablation] ✓ Layer 0: Query propagation enabled (no adapter needed)
[CMQE Ablation] ✓ Layer 1: Hierarchical queries enabled with scale 1.500
[CMQE Ablation] ✓ Layer 1: Query propagation enabled with adapter (64->57)
[CMQE Ablation] ✓ Layer 2: Hierarchical queries enabled with scale 2.000
[CMQE Ablation] ✓ Layer 2: Query propagation enabled with adapter (57->51)
[CMQE Ablation] ✓ Layer 3: Hierarchical queries enabled with scale 2.500
[CMQE Ablation] ✓ Layer 3: Query propagation enabled with adapter (51->44)
[CMQE Ablation] ✓ Adaptive fusion enabled
[CMQE Ablation] ✓ Hierarchical queries: [64, 57, 51, 44]
[CMQE Ablation] ✓ Layer 0: Hierarchical queries enabled with scale 1.000
[CMQE Ablation] ✓ Layer 0: Query propagation enabled (no adapter needed)
[CMQE Ablation] ✓ Layer 1: Hierarchical queries enabled with scale 1.500
[CMQE Ablation] ✓ Layer 1: Query propagation enabled with adapter (64->57)
[CMQE Ablation] ✓ Layer 2: Hierarchical queries enabled with scale 2.000
[CMQE Ablation] ✓ Layer 2: Query propagation enabled with adapter (57->51)
[CMQE Ablation] ✓ Layer 3: Hierarchical queries enabled with scale 2.500
[CMQE Ablation] ✓ Layer 3: Query propagation enabled with adapter (51->44)
[CMQE Ablation] ✓ Adaptive fusion enabled
[CMQE Ablation] ✓ Hierarchical queries: [76, 68, 60, 53]
[CMQE Ablation] ✓ Layer 0: Hierarchical queries enabled with scale 1.000
[CMQE Ablation] ✓ Layer 0: Query propagation enabled (no adapter needed)
[CMQE Ablation] ✓ Layer 1: Hierarchical queries enabled with scale 1.500
[CMQE Ablation] ✓ Layer 1: Query propagation enabled with adapter (76->68)
[CMQE Ablation] ✓ Layer 2: Hierarchical queries enabled with scale 2.000
[CMQE Ablation] ✓ Layer 2: Query propagation enabled with adapter (68->60)
[CMQE Ablation] ✓ Layer 3: Hierarchical queries enabled with scale 2.500
[CMQE Ablation] ✓ Layer 3: Query propagation enabled with adapter (60->53)
[CMQE Ablation] ✓ Adaptive fusion enabled
[CMQE Ablation] ✓ Hierarchical queries: [76, 68, 60, 53]
[CMQE Ablation] ✓ Layer 0: Hierarchical queries enabled with scale 1.000
[CMQE Ablation] ✓ Layer 0: Query propagation enabled (no adapter needed)
[CMQE Ablation] ✓ Layer 1: Hierarchical queries enabled with scale 1.500
[CMQE Ablation] ✓ Layer 1: Query propagation enabled with adapter (76->68)
[CMQE Ablation] ✓ Layer 2: Hierarchical queries enabled with scale 2.000
[CMQE Ablation] ✓ Layer 2: Query propagation enabled with adapter (68->60)
[CMQE Ablation] ✓ Layer 3: Hierarchical queries enabled with scale 2.500
[CMQE Ablation] ✓ Layer 3: Query propagation enabled with adapter (60->53)
[CMQE Ablation] ✓ Adaptive fusion enabled
[CMQE Ablation] ✓ Hierarchical queries: [76, 68, 60, 53]
[CMQE Ablation] ✓ Layer 0: Hierarchical queries enabled with scale 1.000
[CMQE Ablation] ✓ Layer 0: Query propagation enabled (no adapter needed)
[CMQE Ablation] ✓ Layer 1: Hierarchical queries enabled with scale 1.500
[CMQE Ablation] ✓ Layer 1: Query propagation enabled with adapter (76->68)
[CMQE Ablation] ✓ Layer 2: Hierarchical queries enabled with scale 2.000
[CMQE Ablation] ✓ Layer 2: Query propagation enabled with adapter (68->60)
[CMQE Ablation] ✓ Layer 3: Hierarchical queries enabled with scale 2.500
[CMQE Ablation] ✓ Layer 3: Query propagation enabled with adapter (60->53)
[CMQE Ablation] ✓ Adaptive fusion enabled
[CMQE Ablation] ✓ Hierarchical queries: [96, 86, 76, 67]
[CMQE Ablation] ✓ Layer 0: Hierarchical queries enabled with scale 1.000
[CMQE Ablation] ✓ Layer 0: Query propagation enabled (no adapter needed)
[CMQE Ablation] ✓ Layer 1: Hierarchical queries enabled with scale 1.500
[CMQE Ablation] ✓ Layer 1: Query propagation enabled with adapter (96->86)
[CMQE Ablation] ✓ Layer 2: Hierarchical queries enabled with scale 2.000
[CMQE Ablation] ✓ Layer 2: Query propagation enabled with adapter (86->76)
[CMQE Ablation] ✓ Layer 3: Hierarchical queries enabled with scale 2.500
[CMQE Ablation] ✓ Layer 3: Query propagation enabled with adapter (76->67)
[CMQE Ablation] ✓ Adaptive fusion enabled
[CMQE Ablation] ✓ CMQE (Cross-Modal Query Exchange) enabled
[EDA Ablation] Using configuration: AMW_MSOF_LTS_ROC
[EDA Ablation] ✓ AMW (Adaptive Modal Weighting) enabled
[EDA Ablation] ✓ MSOF (Multi-Scale Offset Fusion) enabled
[EDA Ablation] ✓ LTS (Learnable Temperature Scaling) enabled
[EDA Ablation] ✓ ROC (Residual Offset Connection) enabled
===========Building DeMo===========
2025-06-05 15:08:51,072 DeMo INFO: DeMo(
  (BACKBONE): build_transformer(
    (base): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (pool): AdaptiveAvgPool1d(output_size=1)
  (rgb_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (nir_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (tir_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (generalFusion): GeneralFusion(
    (cmqe_system): CMQEModalitySystem(
      (boq_modules): ModuleDict(
        (RGB): CMQEAdaptiveBoQ(
          (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (pos_encoding): CMQEPositionalEncoding()
          (boqs): ModuleList(
            (0): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (1): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=64, out_features=57, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (2): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=57, out_features=51, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (3): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=51, out_features=44, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (adaptive_fusion): CMQEAdaptiveFusion(
            (attention_net): Sequential(
              (0): Linear(in_features=512, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=1, bias=True)
            )
            (final_proj): Linear(in_features=216, out_features=1, bias=True)
          )
        )
        (NI): CMQEAdaptiveBoQ(
          (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (pos_encoding): CMQEPositionalEncoding()
          (boqs): ModuleList(
            (0): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (1): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=64, out_features=57, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (2): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=57, out_features=51, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (3): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=51, out_features=44, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (adaptive_fusion): CMQEAdaptiveFusion(
            (attention_net): Sequential(
              (0): Linear(in_features=512, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=1, bias=True)
            )
            (final_proj): Linear(in_features=216, out_features=1, bias=True)
          )
        )
        (TI): CMQEAdaptiveBoQ(
          (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (pos_encoding): CMQEPositionalEncoding()
          (boqs): ModuleList(
            (0): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (1): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=64, out_features=57, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (2): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=57, out_features=51, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (3): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=51, out_features=44, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (adaptive_fusion): CMQEAdaptiveFusion(
            (attention_net): Sequential(
              (0): Linear(in_features=512, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=1, bias=True)
            )
            (final_proj): Linear(in_features=216, out_features=1, bias=True)
          )
        )
        (RGB_NI): CMQEAdaptiveBoQ(
          (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (pos_encoding): CMQEPositionalEncoding()
          (boqs): ModuleList(
            (0): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (1): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=76, out_features=68, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (2): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=68, out_features=60, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (3): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=60, out_features=53, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (adaptive_fusion): CMQEAdaptiveFusion(
            (attention_net): Sequential(
              (0): Linear(in_features=512, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=1, bias=True)
            )
            (final_proj): Linear(in_features=257, out_features=1, bias=True)
          )
        )
        (RGB_TI): CMQEAdaptiveBoQ(
          (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (pos_encoding): CMQEPositionalEncoding()
          (boqs): ModuleList(
            (0): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (1): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=76, out_features=68, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (2): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=68, out_features=60, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (3): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=60, out_features=53, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (adaptive_fusion): CMQEAdaptiveFusion(
            (attention_net): Sequential(
              (0): Linear(in_features=512, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=1, bias=True)
            )
            (final_proj): Linear(in_features=257, out_features=1, bias=True)
          )
        )
        (NI_TI): CMQEAdaptiveBoQ(
          (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (pos_encoding): CMQEPositionalEncoding()
          (boqs): ModuleList(
            (0): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (1): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=76, out_features=68, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (2): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=68, out_features=60, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (3): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=60, out_features=53, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (adaptive_fusion): CMQEAdaptiveFusion(
            (attention_net): Sequential(
              (0): Linear(in_features=512, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=1, bias=True)
            )
            (final_proj): Linear(in_features=257, out_features=1, bias=True)
          )
        )
        (RGB_NI_TI): CMQEAdaptiveBoQ(
          (norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (pos_encoding): CMQEPositionalEncoding()
          (boqs): ModuleList(
            (0): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (1): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=96, out_features=86, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (2): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=86, out_features=76, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
            (3): CMQEBoQBlock(
              (encoder): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.0, inplace=False)
                (dropout2): Dropout(p=0.0, inplace=False)
              )
              (query_projection): Linear(in_features=512, out_features=512, bias=True)
              (query_adapter): Linear(in_features=76, out_features=67, bias=True)
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (cross_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
              )
              (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (adaptive_fusion): CMQEAdaptiveFusion(
            (attention_net): Sequential(
              (0): Linear(in_features=512, out_features=128, bias=True)
              (1): ReLU()
              (2): Linear(in_features=128, out_features=1, bias=True)
            )
            (final_proj): Linear(in_features=325, out_features=1, bias=True)
          )
        )
      )
      (cmqe_module): CrossModalQueryExchange(
        (cross_modal_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (query_fusion): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=512, out_features=512, bias=True)
        )
        (query_adapters): ModuleDict()
      )
    )
    (deformselectablation): DAttentionEnhancedAblation(
      (modal_gate): Sequential(
        (0): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))
        (3): Sigmoid()
      )
      (conv_offset): Sequential(
        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), groups=512)
        (3): GELU(approximate='none')
        (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (conv_offset_coarse): Sequential(
        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(512, 512, kernel_size=(6, 6), stride=(2, 2), padding=(1, 1), groups=512)
        (3): GELU(approximate='none')
        (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (conv_offset_fine): Sequential(
        (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2), groups=512)
        (3): GELU(approximate='none')
        (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (proj_q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attn_drop): Dropout(p=0.0, inplace=False)
    )
  )
  (classifier_moe): Linear(in_features=3584, out_features=171, bias=False)
  (bottleneck_moe): BatchNorm1d(3584, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier): Linear(in_features=1536, out_features=171, bias=False)
  (bottleneck): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
2025-06-05 15:08:51,075 DeMo INFO: number of parameters:250.045303
using soft triplet loss for training
label smooth on, numclasses: 171
2025-06-05 15:08:51,312 DeMo.train INFO: start training
diversityweight: 0 mxa
/home/ma1/anaconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/ma1/anaconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/ma1/anaconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ma1/work/demorelated/DeMo/modeling/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
/home/ma1/anaconda3/envs/DeMo/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
2025-06-05 15:09:09,999 DeMo.train INFO: Epoch[1] Iteration[10/54] Loss: 4.675, Acc: 0.014, Base Lr: 6.65e-05
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
2025-06-05 15:09:15,913 DeMo.train INFO: Epoch[1] Iteration[20/54] Loss: 4.369, Acc: 0.029, Base Lr: 6.65e-05
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
2025-06-05 15:09:21,983 DeMo.train INFO: Epoch[1] Iteration[30/54] Loss: 4.229, Acc: 0.051, Base Lr: 6.65e-05
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
2025-06-05 15:09:28,002 DeMo.train INFO: Epoch[1] Iteration[40/54] Loss: 4.146, Acc: 0.070, Base Lr: 6.65e-05
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
2025-06-05 15:09:34,041 DeMo.train INFO: Epoch[1] Iteration[50/54] Loss: 4.087, Acc: 0.086, Base Lr: 6.65e-05
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
2025-06-05 15:09:36,008 DeMo.train INFO: Epoch 1 done. Time per batch: 0.841[s] Speed: 76.1[samples/s]
2025-06-05 15:09:36,012 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-05 15:09:36,013 DeMo.train INFO: Current is the ori feature testing!
2025-06-05 15:09:36,013 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.1835
CMC curve: Rank-1: 0.1567, Rank-5: 0.2859, Rank-10: 0.3756
=> Enter reranking
=> Results after reranking:
mAP: 0.2544
CMC curve: Rank-1: 0.1986, Rank-5: 0.2500, Rank-10: 0.3182
=> Improvement:
mAP improvement: 0.0709 (+38.63%)
Rank-1 improvement: 0.0419 (+26.72%)
tstpic False mxa
2025-06-05 15:09:57,483 DeMo.train INFO: Validation Results - Epoch: 1
2025-06-05 15:09:57,484 DeMo.train INFO: mAP: 25.4%
2025-06-05 15:09:57,484 DeMo.train INFO: CMC curve, Rank-1  :19.9%
2025-06-05 15:09:57,484 DeMo.train INFO: CMC curve, Rank-5  :25.0%
2025-06-05 15:09:57,484 DeMo.train INFO: CMC curve, Rank-10 :31.8%
2025-06-05 15:09:57,484 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-05 15:09:57,488 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-05 15:09:57,488 DeMo.train INFO: Current is the moe feature testing!
2025-06-05 15:09:57,488 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/ma1/work/demorelated/DeMo/utils/reranking.py:40: UserWarning: This overload of addmm_ is deprecated:
	addmm_(Number beta, Number alpha, Tensor mat1, Tensor mat2)
Consider using one of the following signatures instead:
	addmm_(Tensor mat1, Tensor mat2, *, Number beta, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)
  distmat.addmm_(1, -2, feat, feat.t())
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.2491
CMC curve: Rank-1: 0.2297, Rank-5: 0.3780, Rank-10: 0.4773
=> Enter reranking
=> Results after reranking:
mAP: 0.3327
CMC curve: Rank-1: 0.2667, Rank-5: 0.2895, Rank-10: 0.3301
=> Improvement:
mAP improvement: 0.0835 (+33.53%)
Rank-1 improvement: 0.0371 (+16.15%)
tstpic False mxa
2025-06-05 15:10:17,242 DeMo.train INFO: Validation Results - Epoch: 1
2025-06-05 15:10:17,242 DeMo.train INFO: mAP: 33.3%
2025-06-05 15:10:17,242 DeMo.train INFO: CMC curve, Rank-1  :26.7%
2025-06-05 15:10:17,242 DeMo.train INFO: CMC curve, Rank-5  :28.9%
2025-06-05 15:10:17,242 DeMo.train INFO: CMC curve, Rank-10 :33.0%
2025-06-05 15:10:17,242 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-05 15:10:18,189 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-05 15:10:18,190 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-06-05 15:10:18,190 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
The test feature is normalized
=> Computing DistMat with euclidean_distance (before reranking)
=> Results before reranking:
mAP: 0.1865
CMC curve: Rank-1: 0.1591, Rank-5: 0.2835, Rank-10: 0.3780
=> Enter reranking
=> Results after reranking:
mAP: 0.2569
CMC curve: Rank-1: 0.1986, Rank-5: 0.2488, Rank-10: 0.3134
=> Improvement:
mAP improvement: 0.0705 (+37.80%)
Rank-1 improvement: 0.0395 (+24.81%)
tstpic False mxa
2025-06-05 15:10:38,056 DeMo.train INFO: Validation Results - Epoch: 1
2025-06-05 15:10:38,056 DeMo.train INFO: mAP: 25.7%
2025-06-05 15:10:38,056 DeMo.train INFO: CMC curve, Rank-1  :19.9%
2025-06-05 15:10:38,057 DeMo.train INFO: CMC curve, Rank-5  :24.9%
2025-06-05 15:10:38,057 DeMo.train INFO: CMC curve, Rank-10 :31.3%
2025-06-05 15:10:38,057 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-05 15:10:47,641 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-06-05 15:10:47,642 DeMo.train INFO: Best mAP: 25.7%
2025-06-05 15:10:47,642 DeMo.train INFO: Best Rank-1: 19.9%
2025-06-05 15:10:47,642 DeMo.train INFO: Best Rank-5: 24.9%
2025-06-05 15:10:47,642 DeMo.train INFO: Best Rank-10: 31.3%
2025-06-05 15:10:47,642 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
[CMQE] Enhanced queries for 7 modalities
2025-06-05 15:10:56,780 DeMo.train INFO: Epoch[2] Iteration[10/54] Loss: 3.513, Acc: 0.250, Base Lr: 9.80e-05
